{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/irs38/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Downloading: 100%|██████████| 694/694 [00:00<00:00, 2.89MB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 5.94MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 5.44MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.65MB/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel, AutoModel, LongformerModel\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from scipy.special import softmax\n",
    "from transformers import BertConfig\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"allenai/longformer-base-4096\"\n",
    "#model = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "model_directory_path = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/results/Outcome/joint_model/longformer/facts/be7001606980465a883b6119ced5fb0d/\"\n",
    "model_path = model_directory_path+\"model.pt\"\n",
    "model = torch.load(model_path)\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "#config = BertConfig.from_pretrained(model_directory_path, output_hidden_states=True)\n",
    "#model = BertModel.from_pretrained(model_path, config=config)\n",
    "#bert_model = TFBertModel.from_pretrained(\"name_or_path_of_model\", config=config)\n",
    "\n",
    "#archive = load_archive(model_path + '/model.tar.gz')\n",
    "#print(archive.config)\n",
    "#archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "#archive.config['model']['output_hidden_states'] = True\n",
    "#model = archive.model\n",
    "#model._output_hidden_states = True\n",
    "#predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "def make_loader(input, mask, labels, claims, train=True):\n",
    "    labels = torch.tensor(labels)\n",
    "    claims = torch.tensor(claims)\n",
    "    data = TensorDataset(input, mask, labels, claims)\n",
    "    if train:\n",
    "        sampler = RandomSampler(data)\n",
    "    else:\n",
    "        sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
    "    return dataloader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "#    label2index = json.load(f)\n",
    "#    index2label = {label2index[k]: k for k in label2index}\n",
    "#label2index\n",
    "# [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "\n",
    "label2index = {\"not_claimed\":0, \"claimed_and_violated\":1, \"claimed_not_violated\":2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data, tokenizer, max=512):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        sent = \" \".join(sent)\n",
    "        sent = sent[:500000] # Speeds the process up for documents with a lot of precedent we would truncate anyway.\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            # return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append([encoded_sent.get('input_ids')])\n",
    "        attention_masks.append([encoded_sent.get('attention_mask')])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "\n",
    "def precisionAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recallAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def meanPrecisionAtK(actual, predicted, k):\n",
    "    return np.mean([precisionAtK(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def meanRecallAtK(actual, predicted, k):\n",
    "    return np.mean([recallAtK(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.4.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "tokenized_dir = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/ECHR/Outcome/longformer\"\n",
    "\n",
    "with open(tokenized_dir + \"/tokenized_dev.pkl\", \"rb\") as f:\n",
    "            val_facts, val_masks, val_arguments, \\\n",
    "            val_masks_arguments, val_ids, val_claims, val_outcomes, _ = pickle.load(f)\n",
    "\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/train.jsonl\", \"r\") as f:\n",
    "    train_Chalkidis_data = [json.loads(line) for line in f]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/dev.jsonl\", \"r\") as f:\n",
    "    dev_Chalkidis_data = [json.loads(line) for line in f]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/test.jsonl\", \"r\") as f:\n",
    "    test_Chalkidis_data = [json.loads(line) for line in f]\n",
    "ids = [item[\"case_no\"] for item in train_Chalkidis_data+dev_Chalkidis_data+test_Chalkidis_data]\n",
    "exs = train_Chalkidis_data+dev_Chalkidis_data+test_Chalkidis_data\n",
    "\n",
    "max_len=512\n",
    "test_size = 100000\n",
    "val_inputs = val_facts\n",
    "val_inputs, val_masks = val_inputs[:test_size, :, :max_len], val_masks[:test_size, :, :max_len]\n",
    "neg_val_labels = val_claims[:test_size, :] - val_outcomes[:test_size, :]\n",
    "pos_val_labels = val_outcomes[:test_size, :]\n",
    "pos_val_labels[pos_val_labels < 0] = 0\n",
    "neg_val_labels[neg_val_labels < 0] = 0\n",
    "val_labels = np.concatenate((pos_val_labels, neg_val_labels), axis=1)\n",
    "claim_val_labels = val_claims[:test_size, :]\n",
    "\n",
    "val_dataloader = make_loader(val_inputs, val_masks, val_labels, claim_val_labels, train=False)\n",
    "\n",
    "dev_data = []\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "    b_input_ids, b_attn_mask, b_labels, b_claims = tuple(t.to(\"cuda\") for t in batch)\n",
    "    b_input_ids = b_input_ids.squeeze(1)\n",
    "    b_attn_mask = b_attn_mask.squeeze(1)\n",
    "    global_attention_mask = torch.zeros(b_input_ids.shape, dtype=torch.long, device=\"cuda\")\n",
    "    global_attention_mask[:, [0]] = 1\n",
    "    dev_data.append([b_input_ids, b_attn_mask, b_labels, b_claims, global_attention_mask])\n",
    "\n",
    "articles = ['10', '11', '13', '14', '18', '2', '3', '4', '5', '6', '7', '8', '9', 'P1-1', 'P4-2', 'P7-1', 'P7-4']\n",
    "\n",
    "#ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "#ex = {\"facts\": \"4.  The applicant was born in 1960 and lives in Oleksandrivka, Kirovograd Region. 5.  On 3 February 2007 the applicant was assaulted. According to the subsequent findings of medical experts, she sustained haematomas on her jaw, shoulder and hip, a bruise under her right eye, concussion, and a displaced rib fracture. The applicant alleges that her assailants were Mr and Mrs K., her daughter\\u2019s former parents-in-law, whereas the domestic authorities found that it was only Mrs K. who had assaulted the applicant. The incident occurred in front of the applicant\\u2019s two-year-old granddaughter. 6.  On 4 February 2007 the applicant lodged a complaint with the police. 7.  On 5 February 2007 a forensic medical expert examined the applicant. He found that she had haematomas which he classified as \\u201cminor bodily injuries\\u201d. 8.  On 14 February 2007 the Oleksandrivka District Police Department (\\u201cthe Oleksandrivka police\\u201d) refused to institute criminal proceedings in connection with the incident. 9.  On 22 February 2007 a forensic medical examination of the applicant was carried out. The expert found that in addition to the previously noted haematomas, the applicant had also suffered concussion and a displaced rib fracture. The expert classified the injuries as \\u201cbodily harm of medium severity\\u201d. 10.  On 20 March 2007 the Oleksandrivka prosecutor overruled the decision of 14 February 2007 as premature and on 21 March 2007 instituted criminal proceedings in connection with the infliction of bodily harm of medium severity on the applicant. 11.  On 20 May 2007 the investigator suspended the investigation for failure to identify the perpetrator. 12.  On 29 August and 3 October 2007 the Oleksandrivka prosecutor\\u2019s office issued two decisions in which it overruled the investigator\\u2019s decision of 20 May 2007 as premature. 13.  On 6 October 2007 the investigator questioned Mr and Mrs K. 14.  On 1 December 2007 the investigator again suspended the investigation for failure to identify the perpetrator. 15.  On 10 December 2007 the Oleksandrivka prosecutor\\u2019s office, in response to the applicant\\u2019s complaint about the progress of the investigation, asked the Kirovograd Regional Police Department to have the police officers in charge of the investigation disciplined. 16.  On 21 January 2008 the Kirovograd Regional Police Department instructed the Oleksandrivka police to immediately resume the investigation. 17.  On 7 April 2008 the investigator decided to ask a forensic medical expert to determine the degree of gravity of the applicant\\u2019s injuries. On 22 September 2008 the expert drew up a report generally confirming the findings of 22 February 2007. 18.  On 15 May 2008 the Kirovograd Regional Police Department informed the applicant that the police officers in charge of the case had been disciplined for omissions in the investigation. 19.  On 23 October 2008 the Oleksandrivka Court absolved Mrs K. from criminal liability under an amnesty law, on the grounds that she had an elderly mother who was dependent on her. On 24 February 2009 the Kirovograd Regional Court of Appeal (\\u201cthe Court of Appeal\\u201d) quashed that judgment, finding no evidence that Mrs K.\\u2019s mother was dependent on her. 20.  On 1 July 2009 the investigator refused to institute criminal proceedings against Mr K. 21.  On 7 July 2009 the Novomyrgorod prosecutor issued a bill of indictment against Mrs K. 22.  On 24 July 2009 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding that the applicant had not been informed about the completion of the investigation until 3 July 2009 and had therefore not been given enough time to study the case file. It also held that the refusal to institute criminal proceedings against Mr K. had contravened the law. 23.  On 13 November 2009 the Novomyrgorod prosecutor quashed the decision of 1 July 2009 not to institute criminal proceedings against Mr K. Subsequently the investigator again refused to institute criminal proceedings against Mr K. 24.  On 21 December 2009 the new round of pre-trial investigation in the case against Mrs K. was completed and another bill of indictment was issued by the Novomyrgorod prosecutor. 25.  On 29 March 2010 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding in particular that the decision not to institute criminal proceedings against Mr K. had been premature, since his role in the incident had not been sufficiently clarified. 26.  On 13 July 2010 the Novomyrgorod prosecutor quashed the decision not to institute criminal proceedings against Mr K. On 26 May 2011 the investigator again refused to institute criminal proceedings against Mr K. 27.  On 20 December 2011 the Znamyanka Court convicted Mrs K. of inflicting bodily harm of medium severity on the applicant, sentencing her to restriction of liberty for two years, suspended for a one-year probationary period. The court found that the decision not to institute criminal proceedings against Mr K. in connection with the same incident had been correct. Mrs K., the prosecutor and the applicant appealed. 28.  On 6 March 2012 the Court of Appeal quashed the judgment and discontinued the criminal proceedings against Mrs K. as time-barred.\", \"claims\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"27454/11\"}\n",
    "\n",
    "#shuffle val_data\n",
    "#random.shuffle(dev_data)\n",
    "\n",
    "interesting_items = []\n",
    "\n",
    "train_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/train_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        train_silver_rationales.append(line.strip())\n",
    "dev_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/dev_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        dev_silver_rationales.append(line.strip())\n",
    "test_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/test_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_silver_rationales.append(line.strip())\n",
    "silver_rationales = train_silver_rationales + dev_silver_rationales + test_silver_rationales\n",
    "\n",
    "ids_to_rationales = {}\n",
    "for id,rationale in zip(ids, silver_rationales):\n",
    "    ids_to_rationales[id] = rationale\n",
    "ids_to_ex = {}\n",
    "for id,ex in zip(ids, exs):\n",
    "    ids_to_ex[id] = ex\n",
    "\n",
    "model.eval()\n",
    "\n",
    "non_zero = 0\n",
    "for i,item in enumerate(dev_data): \n",
    "    b_input_ids, b_attn_mask, b_labels, b_claims, global_attention_mask = item\n",
    "    logits, last_hidden_state_cls = model(b_input_ids.cuda(), b_attn_mask.cuda(), global_attention_mask, b_claims) #predictor.predict_json(e)\n",
    "    logits = logits.reshape(b_input_ids.shape[0], -1, 3)\n",
    "    claims = b_claims\n",
    "    gold = b_labels\n",
    "    D_out = int(b_labels.shape[1] / 2)\n",
    "    y = torch.zeros(b_labels.shape[0], D_out).long().to(\"cuda\")\n",
    "    y[b_labels[:, :D_out].bool()] = 1\n",
    "    y[b_labels[:, D_out:].bool()] = 2\n",
    "    y = y.squeeze(1)\n",
    "    out = torch.argmax(logits, dim=2).squeeze(1)\n",
    "    if \";\" not in val_ids[i]:\n",
    "        gold_id = val_ids[i]\n",
    "    else:\n",
    "         for id in val_ids[i].split(\";\"):\n",
    "            if id in ids:\n",
    "                gold_id = id\n",
    "                break\n",
    "    silver_rat = ids_to_rationales[gold_id]\n",
    "    ex = ids_to_ex[gold_id]\n",
    "    #print(\"Q\"*100)\n",
    "    #print(torch.equal(out, y))\n",
    "    #print(out.sum != 0)\n",
    "    #print(silver_rat != [])\n",
    "    if silver_rat != [] and silver_rat != \"[]\": # out.sum != 0 and torch.equal(out, y)\n",
    "        rats = [int(num) for num in silver_rat.lstrip(\"[\").rstrip(\"]\").split(\",\")]\n",
    "        non_zero += 1\n",
    "        interesting_items.append({\"out\":out, \"ex\":ex, \"claims\":claims, \"gold\":gold, \"y\":y, \"silver_rationales\":rats})\n",
    "        #break\n",
    "print(non_zero)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_sentences = False # otherwise masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m global_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(preprocessed_masked_ex\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m global_attention_mask[:, [\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 80\u001b[0m last_hidden_state_cls \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_masked_ex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_masked_ex_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#masked_out, last_hidden_state_cls = model(preprocessed_masked_ex.squeeze(1).cuda(), preprocessed_masked_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), None)[0] #predictor.predict_json(masked_ex)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#print(\"indices\", m1_i, m2_i)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#print(\"gold labels\", masked_out['labels'])\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#print(\"masked out sentence\", masked_sentence)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m encoded\u001b[38;5;241m.\u001b[39mappend(last_hidden_state_cls\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/contrastive-explanations/model.py:96\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention, claims)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Feed input to BERT\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongformer:\n\u001b[0;32m---> 96\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1594\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1586\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape, device)[\n\u001b[1;32m   1587\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[1;32m   1588\u001b[0m ]\n\u001b[1;32m   1590\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1591\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m   1592\u001b[0m )\n\u001b[0;32m-> 1594\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1602\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1223\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1216\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1217\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         is_index_global_attn,\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1223\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1158\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_masked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m ):\n\u001b[0;32m-> 1158\u001b[0m     self_attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1166\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1103\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_masked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m ):\n\u001b[0;32m-> 1103\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m   1111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:673\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# compute value for global attention and overwrite to attention output\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# TODO: remove the redundant computation\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_global_attn:\n\u001b[0;32m--> 673\u001b[0m     global_attn_output, global_attn_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_global_attn_output_from_hidden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_num_global_attn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_global_attn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_local_index_global_attn_nonzero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local_index_global_attn_nonzero\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn_nonzero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn_nonzero\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_local_index_no_global_attn_nonzero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local_index_no_global_attn_nonzero\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;66;03m# get only non zero global attn output\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     nonzero_global_attn_output \u001b[38;5;241m=\u001b[39m global_attn_output[\n\u001b[1;32m    684\u001b[0m         is_local_index_global_attn_nonzero[\u001b[38;5;241m0\u001b[39m], :, is_local_index_global_attn_nonzero[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    685\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1047\u001b[0m, in \u001b[0;36mLongformerSelfAttention._compute_global_attn_output_from_hidden\u001b[0;34m(self, hidden_states, max_num_global_attn_indices, is_local_index_global_attn_nonzero, is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero, is_index_masked)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# global attn output\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m global_attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(global_attn_probs, global_value_vectors)\n\u001b[0;32m-> 1047\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglobal_attn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_global_attn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_attn_output tensor has the wrong size. Size should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(batch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mmax_num_global_attn_indices,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_attn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1053\u001b[0m global_attn_probs \u001b[38;5;241m=\u001b[39m global_attn_probs\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, max_num_global_attn_indices, seq_len)\n\u001b[1;32m   1054\u001b[0m global_attn_output \u001b[38;5;241m=\u001b[39m global_attn_output\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m   1055\u001b[0m     batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, max_num_global_attn_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m   1056\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "all_interesting_results = []\n",
    "\n",
    "index2label = {0: \"not_claimed\", 1: \"claimed_and_violated\", 2: \"claimed_not_violated\"}\n",
    "\n",
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "\n",
    "for interesting_item in interesting_items: \n",
    "    out = interesting_item[\"out\"]\n",
    "    claims = interesting_item[\"claims\"]\n",
    "    ex = interesting_item[\"ex\"]\n",
    "    y = interesting_item[\"y\"]\n",
    "    facts = ex[\"facts\"]\n",
    "    gold = interesting_item[\"gold\"][0]\n",
    "    silver_rationales = interesting_item[\"silver_rationales\"]\n",
    "    \n",
    "    #print(\"logits.shape\")\n",
    "    #print(logits.shape)\n",
    "    #print(\"out.shape\")\n",
    "    #print(out.shape)\n",
    "    #encoded_orig, _ = preprocessing_for_bert([facts], tokenizer, max=512)\n",
    "\n",
    "    preprocessed_ex, preprocessed_ex_masks = preprocessing_for_bert([facts], tokenizer, max=512)\n",
    "    global_attention_mask = torch.zeros(preprocessed_ex.shape, dtype=torch.long, device=\"cuda\")\n",
    "    global_attention_mask[:, [0]] = 1\n",
    "    #print(\"Q\"*100)\n",
    "    #ou = model.model(preprocessed_ex.squeeze(1).cuda(), preprocessed_ex_masks.squeeze(1).cuda())\n",
    "    #print(ou.last_hidden_state.shape)\n",
    "    #print(ou.last_hidden_state[:, 0, :].shape)\n",
    "    #print(ou)\n",
    "    #print(\"Q\"*100)\n",
    "    encoded_orig = model(preprocessed_ex.squeeze(1).cuda(), preprocessed_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), claims)[1]\n",
    "\n",
    "    #print('Predicted: ', facts)\n",
    "\n",
    "    facts_sentences = facts\n",
    "    sentence_lengths = [len(tokenizer.tokenize(sentence)) for sentence in facts_sentences]\n",
    "    tokenized_facts = tokenizer.tokenize(\" \".join(facts_sentences))\n",
    "    number_of_sentences = len(facts_sentences)\n",
    "\n",
    "    masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "    if masking_sentences: \n",
    "        masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "    else: \n",
    "        masks2 = list(all_consecutive_masks2(tokenizer.tokenize(\" \".join(facts_sentences))))\n",
    "    \n",
    "    encoded = []\n",
    "    mask_mapping = []\n",
    "\n",
    "    for m1_i, m1 in enumerate(masks1):\n",
    "        masked1 = []\n",
    "        for i in m1:\n",
    "            masked1[i] = '<mask>'\n",
    "        masked1 = ' '.join(masked1)\n",
    "        masked_sentence = []\n",
    "        for m2_i, m2 in enumerate(masks2):\n",
    "            if masking_sentences: \n",
    "                masked2 = facts_sentences.copy()\n",
    "                for i in m2:\n",
    "                    masked_sentence.append(masked2[i])\n",
    "                    sentence_length = len(tokenizer.tokenize(masked2[i]))\n",
    "                    masked2[i] = '<mask> '*sentence_length\n",
    "                masked2 = tokenizer.tokenize(' '.join(masked2))\n",
    "            else: \n",
    "                masked2 = tokenized_facts.copy()\n",
    "                for i in m2: \n",
    "                    masked2[i] = '<mask>'\n",
    "                masked2 = masked2\n",
    "\n",
    "            masked_ex = {\n",
    "                \"facts\": masked2,\n",
    "                \"claims\": claims,\n",
    "                \"case_no\": ex['case_no']\n",
    "            }\n",
    "            \n",
    "            preprocessed_masked_ex, preprocessed_masked_ex_masks = preprocessing_for_bert([masked_ex[\"facts\"]], tokenizer, max=512)\n",
    "            global_attention_mask = torch.zeros(preprocessed_masked_ex.shape, dtype=torch.long, device=\"cuda\")\n",
    "            global_attention_mask[:, [0]] = 1\n",
    "            last_hidden_state_cls = model(preprocessed_masked_ex.squeeze(1).cuda(), preprocessed_masked_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), claims)[1]\n",
    "            #masked_out, last_hidden_state_cls = model(preprocessed_masked_ex.squeeze(1).cuda(), preprocessed_masked_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), None)[0] #predictor.predict_json(masked_ex)\n",
    "\n",
    "            #print(\"indices\", m1_i, m2_i)\n",
    "            #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "            #print(\"gold labels\", masked_out['labels'])\n",
    "            #print(\"masked out sentence\", masked_sentence)\n",
    "            encoded.append(last_hidden_state_cls.cpu().detach())\n",
    "            mask_mapping.append((m1_i, m2_i))\n",
    "            \n",
    "            #print(\"====\")\n",
    "            \n",
    "    # make a tensor out of a list of tensors\n",
    "    encoded = torch.cat(encoded, dim=0)\n",
    "    encoded = np.array(encoded)\n",
    "\n",
    "    encoded_orig = np.array(encoded_orig.cpu().detach())\n",
    "\n",
    "    # replace some random f in the following list with another option from\n",
    "    # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "    label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    interesting_label_options = [\"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    # choosing an article for which either the predicted or the gold label is one of the interesting ones (not 0)\n",
    "    article_id = gold.index(2) if 2 in gold else random.choice([i for i in range(len(out[0])) if index2label[out[0][i].item()] in interesting_label_options or index2label[y[0][i].item()] in interesting_label_options])\n",
    "    # choosing a foil randomly that is not the fact\n",
    "\n",
    "    fact_id = out[0][article_id].item()\n",
    "    foil_id = random.choice([label2index[i] for i in interesting_label_options if label2index[i] != fact_id])\n",
    "\n",
    "    fact_idx = article_id * len(label_options) + fact_id\n",
    "    foil_idx = article_id * len(label_options) + foil_id\n",
    "\n",
    "    classifier_w = model_state_dict[\"classifier_positive.0.weight\"].cpu().numpy()\n",
    "    classifier_b = model_state_dict[\"classifier_positive.0.bias\"].cpu().numpy()\n",
    "\n",
    "    u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "    contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "    z_all = encoded_orig \n",
    "    z_h = encoded \n",
    "    z_all_row = encoded_orig @ contrastive_projection\n",
    "    z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "    prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "    prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "    prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "    p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "    q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "    p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "    q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "    distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "    highlight_rankings = np.argsort(-distances)\n",
    "    explained_indices = []\n",
    "    explained_distances = []\n",
    "\n",
    "    if masking_sentences: \n",
    "        for i in range(len(facts_sentences)):\n",
    "            rank = highlight_rankings[i]\n",
    "            m1_i, m2_i = mask_mapping[rank]\n",
    "            \n",
    "            masked_sentence = []\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for k in masks2[m2_i]:\n",
    "                masked_sentence.append(masked2[k])\n",
    "                masked2[k] = '<mask>'\n",
    "            explained_indices.append(k)\n",
    "            explained_distances.append(distances[rank])\n",
    "            masked2 = ' '.join(masked2)\n",
    "    else: \n",
    "        for i in range(len(tokenized_facts)):\n",
    "            rank = highlight_rankings[i]\n",
    "            m1_i, m2_i = mask_mapping[rank]\n",
    "            masked_sentence = []\n",
    "            masked2 = tokenized_facts.copy()\n",
    "            for k in masks2[m2_i]:\n",
    "                masked_sentence.append(masked2[k])\n",
    "                masked2[k] = '<mask>'\n",
    "            explained_indices.append(k)\n",
    "            explained_distances.append(distances[rank])\n",
    "            masked2 = ' '.join(masked2)\n",
    "\n",
    "    ex_dict = {\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices, \"explained_distances\":explained_distances, \"number_of_sentences\":number_of_sentences, \"article_id\": article_id, \"fact\": index2label[fact_id], \"foil\": index2label[foil_id], \"gold\": y, \"predicted\": out}\n",
    "    \n",
    "    if fact_id == 1:\n",
    "        positive.append(ex_dict)\n",
    "    elif fact_id == 2:\n",
    "        negative.append(ex_dict)\n",
    "    else: \n",
    "        neutral.append(ex_dict)\n",
    "\n",
    "    all_interesting_results.append(ex_dict)\n",
    "    print(ex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 55 169 495\n",
      "ALL:\n",
      "271\n",
      "spearman\n",
      "-0.08075351839472075 5.98311702112927e-13\n",
      "kendall\n",
      "-0.06611596874064848 6.489359371015991e-13\n",
      "0:\n",
      "0\n",
      "spearman\n",
      "nan nan\n",
      "kendall\n",
      "nan nan\n",
      "1:\n",
      "0\n",
      "spearman\n",
      "nan nan\n",
      "kendall\n",
      "nan nan\n",
      "2:\n",
      "271\n",
      "spearman\n",
      "-0.08075351839472075 5.98311702112927e-13\n",
      "kendall\n",
      "-0.06611596874064848 6.489359371015991e-13\n",
      "CORRECT:\n",
      "116\n",
      "spearman\n",
      "-0.11274953769471877 6.355377710395803e-10\n",
      "kendall\n",
      "-0.09279508090930122 7.129700932064468e-10\n",
      "0 CORRECT:\n",
      "0\n",
      "spearman\n",
      "nan nan\n",
      "kendall\n",
      "nan nan\n",
      "1 CORRECT:\n",
      "0\n",
      "spearman\n",
      "nan nan\n",
      "kendall\n",
      "nan nan\n",
      "2 CORRECT:\n",
      "116\n",
      "spearman\n",
      "-0.11274953769471877 6.355377710395803e-10\n",
      "kendall\n",
      "-0.09279508090930122 7.129700932064468e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2958552/2421072113.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if torch.equal(torch.tensor(predicted), torch.tensor(gold)):\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "print(len(positive), len(negative), len(neutral), len(all_interesting_results))\n",
    "\n",
    "results = positive\n",
    "\n",
    "indices = [a[\"explained_indices\"] for a in results]\n",
    "distances = [a[\"explained_distances\"] for a in results]\n",
    "rationales = [p[\"silver_rationales\"] for p in results]\n",
    "number_of_sentences = [n[\"number_of_sentences\"] for n in results]\n",
    "golds = [g[\"gold\"] for g in results]\n",
    "article_ids = [a[\"article_id\"] for a in results]\n",
    "predicteds = [a[\"predicted\"] for a in results]\n",
    "foil_ids = [label2index[a[\"foil\"]] for a in results]\n",
    "\n",
    "all_distances = []\n",
    "all_rationales = []\n",
    "distances0 = []\n",
    "rationales0 = []\n",
    "distances1 = []\n",
    "rationales1 = []\n",
    "distances2 = []\n",
    "rationales2 = []\n",
    "distances_correct = []\n",
    "rationales_correct = []\n",
    "distances0_correct = []\n",
    "rationales0_correct = []\n",
    "distances1_correct = []\n",
    "rationales1_correct = []\n",
    "distances2_correct = []\n",
    "rationales2_correct = []\n",
    "\n",
    "for rationale, distance, number, index, gold, article_id, predicted, foil_id in zip(rationales, distances, number_of_sentences, indices, golds, article_ids, predicteds, foil_ids):\n",
    "    gold_label = gold[0][article_id]\n",
    "    item_distances = []\n",
    "    item_rationales = []\n",
    "    for i in range(number):\n",
    "        item_distances.append(distance[index.index(i)])\n",
    "        if i in rationale: \n",
    "            item_rationales.append(1)\n",
    "        else:\n",
    "            item_rationales.append(0)\n",
    "    if foil_id == 0:\n",
    "        distances0.append(item_distances)\n",
    "        rationales0.append(item_rationales)\n",
    "    elif foil_id == 1:\n",
    "        distances1.append(item_distances)\n",
    "        rationales1.append(item_rationales)\n",
    "    elif foil_id == 2:\n",
    "        distances2.append(item_distances)\n",
    "        rationales2.append(item_rationales)\n",
    "    if torch.equal(torch.tensor(predicted), torch.tensor(gold)):\n",
    "        distances_correct.append(item_distances)\n",
    "        rationales_correct.append(item_rationales)\n",
    "        if foil_id == 0:\n",
    "            distances0_correct.append(item_distances)\n",
    "            rationales0_correct.append(item_rationales)\n",
    "        elif foil_id == 1:\n",
    "            distances1_correct.append(item_distances)\n",
    "            rationales1_correct.append(item_rationales)\n",
    "        elif foil_id == 2:\n",
    "            distances2_correct.append(item_distances)\n",
    "            rationales2_correct.append(item_rationales)\n",
    "    all_distances.append(item_distances)\n",
    "    all_rationales.append(item_rationales)\n",
    "\n",
    "# normalize distances\n",
    "#all_distances = [[abs(item/max(dist_list)) for item in dist_list] for dist_list in all_distances]\n",
    "#distances0 = [[abs(item/max(dist_list)) for item in dist_list] for dist_list in distances0]\n",
    "#distances1 = [[abs(item/max(dist_list)) for item in dist_list] for dist_list in distances1]\n",
    "#distances2 = [[abs(item/max(dist_list)) for item in dist_list] for dist_list in distances2]\n",
    "#distances_correct = [[abs(item/max(dist_list)) for item in dist_list] for dist_list in distances_correct]\n",
    "\n",
    "print(\"ALL:\")\n",
    "print(len(all_distances))\n",
    "# find pearson and spearman correlation between item_distances and item_rationales\n",
    "coef, p = spearmanr(flatten_list(all_distances), flatten_list(all_rationales))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "# calculate kendall correlation between item_distances and item_rationales\n",
    "coef, p = kendalltau(flatten_list(all_distances), flatten_list(all_rationales))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"0:\")\n",
    "print(len(distances0))\n",
    "coef, p = spearmanr(flatten_list(distances0), flatten_list(rationales0))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances0), flatten_list(rationales0))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"1:\")\n",
    "print(len(distances1))\n",
    "coef, p = spearmanr(flatten_list(distances1), flatten_list(rationales1))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances1), flatten_list(rationales1))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"2:\")\n",
    "print(len(distances2))\n",
    "coef, p = spearmanr(flatten_list(distances2), flatten_list(rationales2))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances2), flatten_list(rationales2))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"CORRECT:\")\n",
    "print(len(distances_correct))\n",
    "coef, p = spearmanr(flatten_list(distances_correct), flatten_list(rationales_correct))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances_correct), flatten_list(rationales_correct))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"0 CORRECT:\")\n",
    "print(len(distances0_correct))\n",
    "coef, p = spearmanr(flatten_list(distances0_correct), flatten_list(rationales0_correct))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances0_correct), flatten_list(rationales0_correct))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"1 CORRECT:\")\n",
    "print(len(distances1_correct))\n",
    "coef, p = spearmanr(flatten_list(distances1_correct), flatten_list(rationales1_correct))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances1_correct), flatten_list(rationales1_correct))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "print(\"2 CORRECT:\")\n",
    "print(len(distances2_correct))\n",
    "coef, p = spearmanr(flatten_list(distances2_correct), flatten_list(rationales2_correct))\n",
    "print(\"spearman\")\n",
    "print(coef, p)\n",
    "coef, p = kendalltau(flatten_list(distances2_correct), flatten_list(rationales2_correct))\n",
    "print(\"kendall\")\n",
    "print(coef, p)\n",
    "\n",
    "\n",
    "#for i in range(2, 10):\n",
    "#    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "#    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n",
    "\n",
    "# correlation between the importance scores of the sentences and the binary score silver rationales (Pearson & Spearman)\n",
    "# check what they have done in the Javoci paper for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "985\n",
      "meanPrecision@ 2   nan\n",
      "meanRecall@ 2   nan\n",
      "meanPrecision@ 3   nan\n",
      "meanRecall@ 3   nan\n",
      "meanPrecision@ 4   nan\n",
      "meanRecall@ 4   nan\n",
      "meanPrecision@ 5   nan\n",
      "meanRecall@ 5   nan\n",
      "meanPrecision@ 6   nan\n",
      "meanRecall@ 6   nan\n",
      "meanPrecision@ 7   nan\n",
      "meanRecall@ 7   nan\n",
      "meanPrecision@ 8   nan\n",
      "meanRecall@ 8   nan\n",
      "meanPrecision@ 9   nan\n",
      "meanRecall@ 9   nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
