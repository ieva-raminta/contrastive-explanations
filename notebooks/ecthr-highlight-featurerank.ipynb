{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params({'evaluate_on_test': True, 'trainer': {'cuda_device': 0, 'learning_rate_scheduler': {'cut_frac': 0.06, 'type': 'slanted_triangular'}, 'num_epochs': 2, 'optimizer': {'lr': 2e-06, 'type': 'huggingface_adamw', 'weight_decay': 0.1}, 'patience': 5, 'use_amp': True, 'validation_metric': '+accuracy'}, 'validation_data_path': 'data/ecthr/Chalkidis/simple_val.jsonl', 'data_loader': {'batch_sampler': {'batch_size': 1, 'type': 'bucket'}}, 'test_data_path': 'data/ecthr/Chalkidis/simple_test.jsonl', 'train_data_path': 'data/ecthr/Chalkidis/simple_train.jsonl', 'model': {'dropout': 0.1, 'feedforward': {'activations': 'tanh', 'hidden_dims': 768, 'input_dim': 768, 'num_layers': 1}, 'namespace': 'tags', 'seq2vec_encoder': {'embedding_dim': 768, 'type': 'cls_pooler'}, 'text_field_embedder': {'token_embedders': {'tokens': {'max_length': 4096, 'model_name': 'allenai/longformer-base-4096', 'type': 'pretrained_transformer'}}}, 'type': 'ecthr'}, 'dataset_reader': {'token_indexers': {'tokens': {'max_length': 4096, 'model_name': 'allenai/longformer-base-4096', 'type': 'pretrained_transformer'}}, 'tokenizer': {'add_special_tokens': False, 'model_name': 'allenai/longformer-base-4096', 'type': 'pretrained_transformer'}, 'type': 'ecthr'}})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'not_claimed': 0, 'claimed_and_violated': 1, 'claimed_not_violated': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"allenai/longformer-base-4096\"\n",
    "model_path=f\"../experiments/models/{DATASET}/{MODEL_NAME}\"\n",
    "\n",
    "archive = load_archive(model_path + '/model.tar.gz')\n",
    "print(archive.config)\n",
    "archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "archive.config['model']['output_hidden_states'] = True\n",
    "model = archive.model\n",
    "model._output_hidden_states = True\n",
    "predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "    label2index = json.load(f)\n",
    "    index2label = {label2index[k]: k for k in label2index}\n",
    "label2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Device index must not be negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m foils \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m c \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m c \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m o \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m c, o \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(claims, outcomes)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#foil = ex['gold_label']\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m out \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict_json(ex)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m encoded_orig \u001b[39m=\u001b[39m out[\u001b[39m'\u001b[39m\u001b[39mencoded_representations\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m fact \u001b[39m=\u001b[39m out[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/predictors/predictor.py:50\u001b[0m, in \u001b[0;36mPredictor.predict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_json\u001b[39m(\u001b[39mself\u001b[39m, inputs: JsonDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JsonDict:\n\u001b[1;32m     49\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_json_to_instance(inputs)\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_instance(instance)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/predictors/predictor.py:189\u001b[0m, in \u001b[0;36mPredictor.predict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_instance\u001b[39m(\u001b[39mself\u001b[39m, instance: Instance) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JsonDict:\n\u001b[0;32m--> 189\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mforward_on_instance(instance)\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m sanitize(outputs)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/models/model.py:151\u001b[0m, in \u001b[0;36mModel.forward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_on_instance\u001b[39m(\u001b[39mself\u001b[39m, instance: Instance) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, numpy\u001b[39m.\u001b[39mndarray]:\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m    Takes an [`Instance`](../data/instance.md), which typically has raw text in it, converts\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m    that text into arrays using this model's [`Vocabulary`](../data/vocabulary.md), passes those\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m    `torch.Tensors` into numpy arrays and remove the batch dimension.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_on_instances([instance])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/models/model.py:177\u001b[0m, in \u001b[0;36mModel.forward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    175\u001b[0m dataset\u001b[39m.\u001b[39mindex_instances(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[1;32m    176\u001b[0m model_input \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mmove_to_device(dataset\u001b[39m.\u001b[39mas_tensor_dict(), cuda_device)\n\u001b[0;32m--> 177\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_output_human_readable(\u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_input))\n\u001b[1;32m    179\u001b[0m instance_separated_output: List[Dict[\u001b[39mstr\u001b[39m, numpy\u001b[39m.\u001b[39mndarray]] \u001b[39m=\u001b[39m [\n\u001b[1;32m    180\u001b[0m     {} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39minstances\n\u001b[1;32m    181\u001b[0m ]\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m name, output \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(outputs\u001b[39m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    723\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_classifier.py:144\u001b[0m, in \u001b[0;36mECtHRClassifier.forward\u001b[0;34m(self, facts, labels)\u001b[0m\n\u001b[1;32m    139\u001b[0m global_attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m    140\u001b[0m     facts[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m global_attention_mask[:, [\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 144\u001b[0m attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmerge_masks(mask, global_attention_mask)\n\u001b[1;32m    146\u001b[0m embedded_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seq2vec_encoder(embedded_text, mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dropout:\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_classifier.py:107\u001b[0m, in \u001b[0;36mECtHRClassifier.merge_masks\u001b[0;34m(self, mask1, mask2)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_masks\u001b[39m(\u001b[39mself\u001b[39m, mask1: torch\u001b[39m.\u001b[39mTensor, mask2: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m mask1 \u001b[39m*\u001b[39m (mask2\u001b[39m.\u001b[39;49mto(mask1\u001b[39m.\u001b[39;49mget_device()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Device index must not be negative"
     ]
    }
   ],
   "source": [
    "\n",
    "ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "\n",
    "claims = ex[\"claims\"]\n",
    "outcomes = ex[\"outcomes\"]\n",
    "foils = [0 if c == 0 else 1 if c == 1 and o == 1 else 2 for c, o in zip(claims, outcomes)]\n",
    "\n",
    "#foil = ex['gold_label']\n",
    "\n",
    "out = predictor.predict_json(ex)\n",
    "encoded_orig = out['encoded_representations']\n",
    "\n",
    "fact = out['label']\n",
    "print('Predicted: ', fact)\n",
    "\n",
    "# assert fact != foil, \"Fact should be different from the foil (if not, pick a different foil)\"\n",
    "\n",
    "ex['sentence1'] = ex['sentence1'].split()\n",
    "ex['sentence2'] = ex['sentence2'].split()\n",
    "\n",
    "tok.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "masks2 = list(all_consecutive_masks2(ex['sentence2'], max_length=1))\n",
    "encoded = []\n",
    "mask_mapping = []\n",
    "preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "for m1_i, m1 in enumerate(masks1):\n",
    "    masked1 = list(ex['sentence1'])\n",
    "    for i in m1:\n",
    "        masked1[i] = '<mask>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "        \n",
    "    for m2_i, m2 in enumerate(masks2):\n",
    "        masked2 = list(ex['sentence2'])\n",
    "        for i in m2:\n",
    "            masked2[i] = '<mask>'\n",
    "        masked2 = ' '.join(masked2)\n",
    "            \n",
    "        masked_ex = {\n",
    "            \"sentence1\": masked1,\n",
    "            \"sentence2\": masked2\n",
    "        }\n",
    "        \n",
    "        masked_out = predictor.predict_json(masked_ex)\n",
    "#         if masked_out['label'] != foil:\n",
    "#             continue\n",
    "        \n",
    "        print(m1_i, m2_i)\n",
    "        print(f\"{masked1}\\n{masked2}\")\n",
    "        print(masked_out['label'])\n",
    "        encoded.append(masked_out['encoded_representations'])\n",
    "        mask_mapping.append((m1_i, m2_i))\n",
    "        \n",
    "        print(\"====\")\n",
    "        \n",
    "encoded = np.array(encoded)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foil = 'neutral'\n",
    "\n",
    "fact_idx = label2index[fact]\n",
    "foil_idx = label2index[foil]\n",
    "print('fact:', index2label[fact_idx])\n",
    "print('foil:', index2label[foil_idx])\n",
    "num_classifiers = 100\n",
    "\n",
    "classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "print(contrastive_projection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.stats import entropy\n",
    "from scipy.special import softmax\n",
    "\n",
    "z_all = encoded_orig \n",
    "z_h = encoded \n",
    "z_all_row = encoded_orig @ contrastive_projection\n",
    "z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "print(' '.join(ex['sentence1']))\n",
    "print(' '.join(ex['sentence2']))\n",
    "\n",
    "print(\"=========\\n=======Farthest masks:=======\")    \n",
    "    \n",
    "highlight_rankings = np.argsort(-distances)\n",
    "\n",
    "for i in range(4):\n",
    "    rank = highlight_rankings[i]\n",
    "    m1_i, m2_i = mask_mapping[rank]\n",
    "    \n",
    "    masked1 = list(ex['sentence1'])\n",
    "    for k in masks1[m1_i]:\n",
    "        masked1[k] = '<m>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "    \n",
    "    masked2 = list(ex['sentence2'])\n",
    "    for k in masks2[m2_i]:\n",
    "        masked2[k] = '<m>'\n",
    "    masked2 = ' '.join(masked2)\n",
    "    \n",
    "    print(masked1)\n",
    "    print(masked2)\n",
    "    print(np.round(distances[rank], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
