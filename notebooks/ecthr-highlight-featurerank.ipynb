{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/irs38/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel, AutoModel, LongformerModel\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from scipy.special import softmax\n",
    "from transformers import BertConfig\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"nlpaueb/legal-bert-base-uncased\"\n",
    "#model = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "model_path = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/results/Outcome/joint_model/legal_bert/facts/11a63ebcd6b242ea9925944f1773188e/model.pt\"\n",
    "model = torch.load(model_path)\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "model_directory_path = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/results/Outcome/joint_model/legal_bert/facts/11a63ebcd6b242ea9925944f1773188e\"\n",
    "\n",
    "#config = BertConfig.from_pretrained(model_directory_path, output_hidden_states=True)\n",
    "#model = BertModel.from_pretrained(model_path, config=config)\n",
    "#bert_model = TFBertModel.from_pretrained(\"name_or_path_of_model\", config=config)\n",
    "\n",
    "#archive = load_archive(model_path + '/model.tar.gz')\n",
    "#print(archive.config)\n",
    "#archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "#archive.config['model']['output_hidden_states'] = True\n",
    "#model = archive.model\n",
    "#model._output_hidden_states = True\n",
    "#predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "def make_loader(input, mask, labels, claims, train=True):\n",
    "    labels = torch.tensor(labels)\n",
    "    claims = torch.tensor(claims)\n",
    "    data = TensorDataset(input, mask, labels, claims)\n",
    "    if train:\n",
    "        sampler = RandomSampler(data)\n",
    "    else:\n",
    "        sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
    "    return dataloader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "#    label2index = json.load(f)\n",
    "#    index2label = {label2index[k]: k for k in label2index}\n",
    "#label2index\n",
    "# [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "\n",
    "label2index = {\"not_claimed\":0, \"claimed_and_violated\":1, \"claimed_not_violated\":2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data, tokenizer, max=512):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        sent = \" \".join(sent)\n",
    "        sent = sent[:500000] # Speeds the process up for documents with a lot of precedent we would truncate anyway.\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            # return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append([encoded_sent.get('input_ids')])\n",
    "        attention_masks.append([encoded_sent.get('attention_mask')])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "\n",
    "def precisionAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recallAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def meanPrecisionAtK(actual, predicted, k):\n",
    "    return np.mean([precisionAtK(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def meanRecallAtK(actual, predicted, k):\n",
    "    return np.mean([recallAtK(a, p, k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.4.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "tokenized_dir = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/ECHR/Outcome/legal_bert\"\n",
    "\n",
    "with open(tokenized_dir + \"/tokenized_dev.pkl\", \"rb\") as f:\n",
    "            val_facts, val_masks, val_arguments, \\\n",
    "            val_masks_arguments, val_ids, val_claims, val_outcomes, _ = pickle.load(f)\n",
    "\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/train.jsonl\", \"r\") as f:\n",
    "    train_Chalkidis_data = [json.loads(line) for line in f]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/dev.jsonl\", \"r\") as f:\n",
    "    dev_Chalkidis_data = [json.loads(line) for line in f]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/test.jsonl\", \"r\") as f:\n",
    "    test_Chalkidis_data = [json.loads(line) for line in f]\n",
    "ids = [item[\"case_no\"] for item in train_Chalkidis_data+dev_Chalkidis_data+test_Chalkidis_data]\n",
    "exs = train_Chalkidis_data+dev_Chalkidis_data+test_Chalkidis_data\n",
    "\n",
    "max_len=512\n",
    "test_size = 100000\n",
    "val_inputs = val_facts\n",
    "val_inputs, val_masks = val_inputs[:test_size, :, :max_len], val_masks[:test_size, :, :max_len]\n",
    "neg_val_labels = val_claims[:test_size, :] - val_outcomes[:test_size, :]\n",
    "pos_val_labels = val_outcomes[:test_size, :]\n",
    "pos_val_labels[pos_val_labels < 0] = 0\n",
    "neg_val_labels[neg_val_labels < 0] = 0\n",
    "val_labels = np.concatenate((pos_val_labels, neg_val_labels), axis=1)\n",
    "claim_val_labels = val_claims[:test_size, :]\n",
    "\n",
    "val_dataloader = make_loader(val_inputs, val_masks, val_labels, claim_val_labels, train=False)\n",
    "\n",
    "dev_data = []\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "    b_input_ids, b_attn_mask, b_labels, b_claims = tuple(t.to(\"cuda\") for t in batch)\n",
    "    b_input_ids = b_input_ids.squeeze(1)\n",
    "    b_attn_mask = b_attn_mask.squeeze(1)\n",
    "    global_attention_mask = torch.zeros(b_input_ids.shape, dtype=torch.long, device=\"cuda\")\n",
    "    global_attention_mask[:, [0]] = 1\n",
    "    dev_data.append([b_input_ids, b_attn_mask, b_labels, b_claims, global_attention_mask])\n",
    "\n",
    "articles = ['10', '11', '13', '14', '18', '2', '3', '4', '5', '6', '7', '8', '9', 'P1-1', 'P4-2', 'P7-1', 'P7-4']\n",
    "\n",
    "#ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "#ex = {\"facts\": \"4.  The applicant was born in 1960 and lives in Oleksandrivka, Kirovograd Region. 5.  On 3 February 2007 the applicant was assaulted. According to the subsequent findings of medical experts, she sustained haematomas on her jaw, shoulder and hip, a bruise under her right eye, concussion, and a displaced rib fracture. The applicant alleges that her assailants were Mr and Mrs K., her daughter\\u2019s former parents-in-law, whereas the domestic authorities found that it was only Mrs K. who had assaulted the applicant. The incident occurred in front of the applicant\\u2019s two-year-old granddaughter. 6.  On 4 February 2007 the applicant lodged a complaint with the police. 7.  On 5 February 2007 a forensic medical expert examined the applicant. He found that she had haematomas which he classified as \\u201cminor bodily injuries\\u201d. 8.  On 14 February 2007 the Oleksandrivka District Police Department (\\u201cthe Oleksandrivka police\\u201d) refused to institute criminal proceedings in connection with the incident. 9.  On 22 February 2007 a forensic medical examination of the applicant was carried out. The expert found that in addition to the previously noted haematomas, the applicant had also suffered concussion and a displaced rib fracture. The expert classified the injuries as \\u201cbodily harm of medium severity\\u201d. 10.  On 20 March 2007 the Oleksandrivka prosecutor overruled the decision of 14 February 2007 as premature and on 21 March 2007 instituted criminal proceedings in connection with the infliction of bodily harm of medium severity on the applicant. 11.  On 20 May 2007 the investigator suspended the investigation for failure to identify the perpetrator. 12.  On 29 August and 3 October 2007 the Oleksandrivka prosecutor\\u2019s office issued two decisions in which it overruled the investigator\\u2019s decision of 20 May 2007 as premature. 13.  On 6 October 2007 the investigator questioned Mr and Mrs K. 14.  On 1 December 2007 the investigator again suspended the investigation for failure to identify the perpetrator. 15.  On 10 December 2007 the Oleksandrivka prosecutor\\u2019s office, in response to the applicant\\u2019s complaint about the progress of the investigation, asked the Kirovograd Regional Police Department to have the police officers in charge of the investigation disciplined. 16.  On 21 January 2008 the Kirovograd Regional Police Department instructed the Oleksandrivka police to immediately resume the investigation. 17.  On 7 April 2008 the investigator decided to ask a forensic medical expert to determine the degree of gravity of the applicant\\u2019s injuries. On 22 September 2008 the expert drew up a report generally confirming the findings of 22 February 2007. 18.  On 15 May 2008 the Kirovograd Regional Police Department informed the applicant that the police officers in charge of the case had been disciplined for omissions in the investigation. 19.  On 23 October 2008 the Oleksandrivka Court absolved Mrs K. from criminal liability under an amnesty law, on the grounds that she had an elderly mother who was dependent on her. On 24 February 2009 the Kirovograd Regional Court of Appeal (\\u201cthe Court of Appeal\\u201d) quashed that judgment, finding no evidence that Mrs K.\\u2019s mother was dependent on her. 20.  On 1 July 2009 the investigator refused to institute criminal proceedings against Mr K. 21.  On 7 July 2009 the Novomyrgorod prosecutor issued a bill of indictment against Mrs K. 22.  On 24 July 2009 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding that the applicant had not been informed about the completion of the investigation until 3 July 2009 and had therefore not been given enough time to study the case file. It also held that the refusal to institute criminal proceedings against Mr K. had contravened the law. 23.  On 13 November 2009 the Novomyrgorod prosecutor quashed the decision of 1 July 2009 not to institute criminal proceedings against Mr K. Subsequently the investigator again refused to institute criminal proceedings against Mr K. 24.  On 21 December 2009 the new round of pre-trial investigation in the case against Mrs K. was completed and another bill of indictment was issued by the Novomyrgorod prosecutor. 25.  On 29 March 2010 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding in particular that the decision not to institute criminal proceedings against Mr K. had been premature, since his role in the incident had not been sufficiently clarified. 26.  On 13 July 2010 the Novomyrgorod prosecutor quashed the decision not to institute criminal proceedings against Mr K. On 26 May 2011 the investigator again refused to institute criminal proceedings against Mr K. 27.  On 20 December 2011 the Znamyanka Court convicted Mrs K. of inflicting bodily harm of medium severity on the applicant, sentencing her to restriction of liberty for two years, suspended for a one-year probationary period. The court found that the decision not to institute criminal proceedings against Mr K. in connection with the same incident had been correct. Mrs K., the prosecutor and the applicant appealed. 28.  On 6 March 2012 the Court of Appeal quashed the judgment and discontinued the criminal proceedings against Mrs K. as time-barred.\", \"claims\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"27454/11\"}\n",
    "\n",
    "#shuffle val_data\n",
    "#random.shuffle(dev_data)\n",
    "\n",
    "interesting_items = []\n",
    "\n",
    "train_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/train_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        train_silver_rationales.append(line.strip())\n",
    "dev_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/dev_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        dev_silver_rationales.append(line.strip())\n",
    "test_silver_rationales = []\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/outcome/test_silver_rationales.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_silver_rationales.append(line.strip())\n",
    "silver_rationales = train_silver_rationales + dev_silver_rationales + test_silver_rationales\n",
    "\n",
    "ids_to_rationales = {}\n",
    "for id,rationale in zip(ids, silver_rationales):\n",
    "    ids_to_rationales[id] = rationale\n",
    "ids_to_ex = {}\n",
    "for id,ex in zip(ids, exs):\n",
    "    ids_to_ex[id] = ex\n",
    "\n",
    "model.eval()\n",
    "\n",
    "non_zero = 0\n",
    "for i,item in enumerate(dev_data): \n",
    "    b_input_ids, b_attn_mask, b_labels, b_claims, global_attention_mask = item\n",
    "    logits, last_hidden_state_cls = model(b_input_ids.cuda(), b_attn_mask.cuda(), global_attention_mask, b_claims) #predictor.predict_json(e)\n",
    "    logits = logits.reshape(b_input_ids.shape[0], -1, 3)\n",
    "    claims = b_claims\n",
    "    gold = b_labels\n",
    "    D_out = int(b_labels.shape[1] / 2)\n",
    "    y = torch.zeros(b_labels.shape[0], D_out).long().to(\"cuda\")\n",
    "    y[b_labels[:, :D_out].bool()] = 1\n",
    "    y[b_labels[:, D_out:].bool()] = 2\n",
    "    y = y.squeeze(1)\n",
    "    out = torch.argmax(logits, dim=2).squeeze(1)\n",
    "    if \";\" not in val_ids[i]:\n",
    "        gold_id = val_ids[i]\n",
    "    else:\n",
    "         for id in val_ids[i].split(\";\"):\n",
    "            if id in ids:\n",
    "                gold_id = id\n",
    "                break\n",
    "    silver_rat = ids_to_rationales[gold_id]\n",
    "    ex = ids_to_ex[gold_id]\n",
    "    #print(\"Q\"*100)\n",
    "    #print(torch.equal(out, y))\n",
    "    #print(out.sum != 0)\n",
    "    #print(silver_rat != [])\n",
    "    if out.sum != 0 and torch.equal(out, y) and silver_rat != []:\n",
    "        non_zero += 1\n",
    "        interesting_items.append({\"out\":out, \"ex\":ex, \"claims\":claims, \"gold\":gold, \"y\":y, \"silver_rationales\":silver_rat})\n",
    "        #break\n",
    "print(non_zero)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape\n",
      "torch.Size([1, 14, 3])\n",
      "out.shape\n",
      "torch.Size([1, 14])\n",
      "QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.5848, -0.2414, -0.3338,  ...,  0.5479,  0.8008,  0.2058],\n",
      "         [-0.2628, -0.1906,  0.1196,  ..., -0.0334,  0.5477,  0.2130],\n",
      "         [-1.0195,  0.1354, -0.1836,  ...,  0.2011,  0.6115, -0.0201],\n",
      "         ...,\n",
      "         [ 0.7113, -0.6428,  0.6852,  ...,  0.3244,  1.0693, -0.1793],\n",
      "         [ 0.3187, -0.6444,  0.5495,  ..., -0.0136,  0.7925, -0.0630],\n",
      "         [ 0.3027,  0.1604, -0.1028,  ...,  0.4215,  1.0229,  0.2662]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 3.4874e-01, -1.8313e-01, -9.9246e-01,  7.6938e-02,  6.8910e-01,\n",
      "          1.7762e-01, -2.4022e-01, -3.2544e-01, -5.1718e-01,  8.5332e-01,\n",
      "          5.7236e-02, -6.7528e-01, -9.0498e-01,  6.8551e-01, -1.0915e-01,\n",
      "          2.1993e-01, -4.1320e-01,  1.2134e-01, -1.5492e-01,  3.5413e-01,\n",
      "         -4.7093e-02,  7.7102e-01, -2.6776e-01,  2.8370e-01, -4.8690e-01,\n",
      "          6.8617e-01,  4.0157e-01, -8.9376e-01, -3.0601e-01, -3.5288e-01,\n",
      "          2.3242e-01, -2.0109e-01,  7.5320e-01,  5.9093e-01, -9.5584e-01,\n",
      "          7.1222e-01, -2.7196e-02,  5.6880e-01,  2.8480e-01,  5.0330e-01,\n",
      "         -4.8345e-01, -4.7212e-01,  4.2753e-01, -2.9882e-01, -8.4775e-01,\n",
      "          1.7671e-01, -7.3190e-01, -5.0005e-01, -1.1132e-01,  9.1426e-01,\n",
      "          9.3190e-01,  9.4743e-01, -4.2608e-01, -3.4539e-01, -7.2720e-01,\n",
      "         -2.5134e-01,  2.2900e-01, -3.4290e-01,  5.3990e-01,  4.7683e-01,\n",
      "         -1.6144e-01, -7.4384e-01,  2.2935e-02, -6.2816e-01,  7.5489e-01,\n",
      "          9.8842e-01,  4.6331e-01,  1.1944e-01,  2.5035e-01,  3.8933e-01,\n",
      "         -2.0629e-01, -5.9627e-01,  5.2645e-01, -9.1365e-02,  9.8840e-01,\n",
      "         -5.7183e-01,  2.2294e-02, -2.2400e-01,  4.3718e-01,  3.6588e-01,\n",
      "          9.8257e-01,  8.3920e-01,  5.9662e-01, -8.9770e-01,  9.8776e-01,\n",
      "          5.2957e-01, -1.2642e-01,  6.4687e-01,  2.6978e-01, -4.3048e-01,\n",
      "          3.3652e-01,  2.2203e-01,  9.5434e-01,  6.8964e-01, -8.8998e-01,\n",
      "         -2.6440e-02,  5.2666e-01, -3.7991e-01,  2.5367e-01, -4.6944e-01,\n",
      "         -2.5879e-01, -2.9289e-01,  2.1636e-01,  1.9683e-01, -4.8440e-01,\n",
      "         -2.8812e-01,  1.1489e-01,  7.4010e-01,  4.6758e-01, -2.5230e-01,\n",
      "          3.2946e-01,  6.6117e-01,  9.6334e-03, -3.6852e-01, -8.4523e-02,\n",
      "          7.5910e-01,  3.7645e-01, -5.0393e-01,  7.4753e-01,  2.2281e-01,\n",
      "          5.0269e-01,  2.2255e-01, -5.6233e-01, -3.2829e-01, -8.8989e-01,\n",
      "          1.5432e-01, -9.1871e-01,  2.4285e-01, -1.4446e-01, -3.6713e-01,\n",
      "         -2.0654e-02,  4.7405e-02,  5.9348e-01,  9.2716e-01,  1.9697e-01,\n",
      "         -8.8502e-01, -5.8760e-01,  8.2610e-01,  2.5852e-01,  4.5613e-01,\n",
      "         -9.5178e-01, -8.4159e-01,  7.4784e-01,  2.2773e-01, -3.4993e-01,\n",
      "          5.3724e-01, -3.6022e-01, -1.8806e-02, -5.2669e-01, -3.8866e-01,\n",
      "         -3.9188e-01,  3.1613e-01,  1.6226e-01, -3.5607e-01,  4.0537e-01,\n",
      "          1.3895e-01, -4.5866e-01, -9.9217e-01, -8.6552e-01,  1.3028e-01,\n",
      "         -2.6515e-02,  3.3055e-01,  1.9481e-01, -4.9956e-01,  5.6165e-01,\n",
      "         -6.4760e-03, -4.2887e-01, -1.4350e-01,  2.4158e-01,  7.6381e-02,\n",
      "         -5.6973e-01,  1.7188e-01, -9.8745e-01,  3.3357e-02,  1.1051e-02,\n",
      "         -1.1741e-01, -9.5490e-01, -1.0030e-01, -1.0018e-01, -2.2332e-01,\n",
      "          5.6826e-02,  1.5445e-01,  5.8348e-01,  2.4994e-01,  5.7056e-01,\n",
      "         -1.9406e-01,  1.0861e-01, -4.7876e-01, -2.5596e-01,  8.4201e-01,\n",
      "          3.5007e-04,  4.5765e-01,  9.2701e-01,  2.8637e-01, -4.7185e-01,\n",
      "         -4.5467e-02, -1.5115e-01, -7.5251e-01,  3.1720e-01, -6.6871e-01,\n",
      "         -1.2550e-01, -9.0450e-01, -1.9270e-01, -4.1322e-01,  9.3531e-01,\n",
      "          4.9746e-01,  8.4359e-01,  3.1559e-01, -1.4496e-01, -5.8052e-01,\n",
      "         -1.1051e-01, -9.8271e-02,  1.1490e-01,  1.6872e-01,  2.4208e-01,\n",
      "          9.2766e-01, -3.3169e-01, -3.6318e-01, -2.6760e-02,  2.0227e-01,\n",
      "          1.3018e-01, -7.1549e-01,  9.6331e-01,  3.3967e-01, -1.0551e-01,\n",
      "         -1.1753e-01,  2.7119e-01, -5.7142e-01, -4.1071e-01,  6.9286e-01,\n",
      "          3.2676e-01,  2.5068e-01, -7.1439e-01, -4.7511e-01, -4.4672e-01,\n",
      "         -8.5010e-01,  3.3211e-01, -5.5506e-01,  1.2045e-01, -1.4202e-01,\n",
      "          2.7662e-01, -7.0500e-01, -6.8723e-01,  3.4543e-02, -4.9684e-01,\n",
      "         -9.6886e-01, -5.0977e-01, -9.1880e-01,  3.1956e-01,  9.6885e-01,\n",
      "          9.6112e-01,  6.2323e-01,  5.2630e-02, -9.7548e-01, -1.2930e-01,\n",
      "         -7.5056e-01,  4.8366e-01,  9.1949e-01, -4.1417e-02,  7.0294e-01,\n",
      "          7.5262e-01,  7.7682e-01, -1.4561e-03,  4.7341e-01,  9.2925e-02,\n",
      "         -1.3069e-01,  3.8275e-01, -4.7392e-01, -3.3920e-01,  3.8976e-02,\n",
      "          5.8758e-01,  1.6040e-01,  4.2673e-02, -5.5327e-02, -1.1241e-01,\n",
      "          4.3429e-01, -9.9753e-01, -3.6838e-01, -3.5156e-01, -2.7542e-01,\n",
      "         -2.1242e-01, -4.0810e-04, -9.9154e-01,  3.5951e-01,  9.4987e-01,\n",
      "         -8.0341e-01,  1.2723e-02, -4.5451e-01, -3.4214e-01, -2.3271e-01,\n",
      "         -6.0815e-01,  9.2055e-02, -6.0671e-01,  7.2181e-01, -2.7593e-02,\n",
      "          9.5454e-02,  3.8135e-01, -4.3197e-01,  4.4170e-01,  4.1337e-01,\n",
      "          4.2686e-01, -6.9458e-01, -6.6414e-01,  4.2432e-01, -6.1960e-01,\n",
      "         -5.7726e-01, -5.0686e-01, -8.9299e-01,  2.4513e-01, -2.6963e-01,\n",
      "          5.9921e-01,  7.6250e-01,  1.1894e-03, -1.9894e-01, -9.9027e-01,\n",
      "         -6.7302e-02, -8.0858e-01, -1.4080e-01, -2.8131e-01,  3.5310e-01,\n",
      "          2.3633e-01, -4.3342e-01, -6.4017e-01,  2.0703e-01,  5.5771e-01,\n",
      "          4.5097e-01,  1.5064e-01,  2.2599e-01,  5.0283e-01,  1.9435e-01,\n",
      "          2.7784e-01,  4.5750e-01, -1.1335e-01,  9.1906e-01,  1.4789e-01,\n",
      "          6.4292e-01,  6.1704e-01,  3.6935e-01,  4.5534e-01,  8.7018e-01,\n",
      "         -7.4594e-01,  7.3371e-01, -9.3187e-01, -4.7391e-01,  6.7984e-01,\n",
      "         -4.0629e-01,  7.0905e-01, -1.5600e-01, -2.4415e-02,  2.4777e-01,\n",
      "          2.8693e-02,  3.2935e-02,  3.3743e-01,  5.8290e-01, -4.1681e-01,\n",
      "         -2.6949e-01,  2.6953e-01,  1.9375e-03, -8.1949e-01,  2.2926e-01,\n",
      "         -7.7109e-01, -6.1772e-01, -9.2988e-01,  8.5456e-01,  4.1835e-02,\n",
      "         -3.9833e-01,  8.8165e-01,  4.8372e-01, -9.6336e-01,  4.1365e-01,\n",
      "          1.7745e-02, -2.7442e-01, -3.1426e-01, -5.1543e-01,  6.8630e-01,\n",
      "          5.4615e-01,  3.7291e-01,  7.8875e-01,  1.7993e-01,  1.5823e-01,\n",
      "         -5.8987e-01, -5.1833e-01, -4.4503e-01, -2.5210e-01,  9.2909e-01,\n",
      "         -3.6809e-02, -1.7333e-01, -1.9887e-01,  6.5387e-01,  1.9432e-01,\n",
      "          8.8828e-01,  1.2196e-01, -4.4450e-01, -1.3920e-01, -1.6460e-01,\n",
      "         -7.2113e-01,  5.4838e-01, -1.3517e-01,  7.2883e-01,  4.8968e-01,\n",
      "         -1.4279e-02,  9.2794e-01,  2.2231e-01,  7.8706e-01,  8.0159e-01,\n",
      "          6.8134e-01, -8.1884e-01, -7.2917e-02,  8.9638e-01, -4.8868e-01,\n",
      "         -4.9811e-01,  5.8634e-02,  7.5917e-02, -5.7673e-01, -6.8482e-02,\n",
      "          9.9053e-01, -5.2470e-01,  8.3919e-02,  6.0369e-01,  1.7590e-01,\n",
      "         -9.8850e-01,  5.4952e-02,  5.3247e-01, -5.9400e-01, -9.5962e-02,\n",
      "          3.6361e-01,  3.2615e-01, -2.0680e-01,  9.3123e-01, -5.4749e-01,\n",
      "         -1.2098e-01,  4.4718e-01,  5.3242e-02,  3.2757e-01, -1.0849e-01,\n",
      "          7.7816e-01, -6.9111e-01, -6.2611e-01, -1.2420e-01,  1.5022e-01,\n",
      "          4.0468e-01, -9.9199e-01,  4.8137e-01,  2.4131e-01,  4.8711e-01,\n",
      "          9.9359e-01, -3.2194e-02, -4.1521e-01,  9.9039e-01, -2.2559e-01,\n",
      "         -8.2354e-01, -5.2729e-01, -2.0958e-01,  6.7411e-01,  2.6423e-01,\n",
      "          7.0397e-01,  5.0864e-01,  7.2438e-01, -7.8730e-01,  4.9615e-02,\n",
      "         -3.3121e-01,  5.8783e-01,  5.8633e-01,  9.2471e-01, -8.3077e-01,\n",
      "         -7.0920e-02,  2.6114e-01, -3.2287e-01,  2.8169e-01,  2.1854e-01,\n",
      "         -5.1557e-01, -9.0757e-01,  7.4851e-02, -6.1301e-01, -3.1709e-01,\n",
      "          9.6437e-01,  4.9829e-01,  8.3034e-01,  4.4100e-01,  7.0023e-02,\n",
      "          9.7286e-01,  5.7071e-02, -9.8290e-01, -7.7396e-01,  1.5588e-01,\n",
      "         -4.5007e-01,  6.7827e-01,  1.4514e-02,  9.8917e-01, -2.5354e-02,\n",
      "         -4.6029e-01,  3.1930e-02,  2.0778e-01,  9.9769e-02, -3.6539e-01,\n",
      "          4.2573e-01,  5.7931e-01, -1.3913e-01,  7.3560e-01,  6.2310e-01,\n",
      "         -6.2885e-01,  1.1494e-03,  1.2301e-01,  3.7361e-01,  7.3999e-01,\n",
      "         -1.8276e-01, -8.2824e-01,  9.7558e-01,  3.2555e-01, -3.3154e-01,\n",
      "          1.2655e-01,  2.3206e-01,  7.9711e-01, -4.2715e-01,  4.0825e-01,\n",
      "          5.0899e-01,  1.4735e-01,  2.1145e-01, -7.5077e-01,  5.2845e-01,\n",
      "          8.5496e-01,  6.6216e-01, -4.2702e-01,  4.5885e-01,  9.0328e-01,\n",
      "          6.6265e-01,  8.7196e-01,  2.9532e-01, -7.2293e-02,  1.2829e-01,\n",
      "         -2.1758e-01, -2.6302e-01,  9.5630e-02,  1.5089e-01, -2.4134e-01,\n",
      "          1.5780e-01, -5.8391e-01,  8.2524e-01,  9.4987e-01,  7.7880e-01,\n",
      "         -9.1345e-01,  9.1717e-01, -2.3925e-01,  8.7327e-01,  2.4922e-01,\n",
      "          8.3738e-01,  2.2518e-01,  8.5955e-01, -6.3055e-01,  6.4952e-02,\n",
      "         -2.3563e-02,  3.4914e-01,  4.2820e-01, -3.4844e-01, -1.2437e-01,\n",
      "          3.1703e-01, -2.4636e-01, -1.7941e-01,  2.5224e-01, -3.4842e-01,\n",
      "         -5.3337e-01, -6.5713e-01,  1.7834e-01,  3.8025e-01,  4.9551e-02,\n",
      "         -5.2734e-01,  2.4406e-01,  1.1778e-01, -2.7802e-01,  3.3589e-02,\n",
      "          3.2153e-01,  4.1213e-01, -4.6956e-01, -6.0045e-01,  4.5602e-03,\n",
      "          1.9800e-01,  2.4493e-01,  8.8825e-01,  3.6368e-01, -1.6149e-01,\n",
      "         -1.5454e-01, -4.5295e-01,  2.2264e-01, -9.8699e-01, -9.5564e-01,\n",
      "         -8.4593e-01,  5.3801e-01, -6.1148e-01,  8.0132e-01, -2.5581e-01,\n",
      "         -1.8293e-02, -4.2416e-01, -2.8868e-01, -1.7921e-01, -7.5402e-01,\n",
      "         -5.3636e-01, -1.3767e-02,  1.7854e-01, -5.5734e-01, -3.8722e-01,\n",
      "          5.3663e-01,  9.6806e-01,  2.1000e-02, -5.3491e-01, -1.8076e-01,\n",
      "         -5.4404e-01, -1.4414e-02, -6.1506e-01, -3.6012e-01, -1.1783e-01,\n",
      "         -9.1411e-01, -4.3343e-01,  1.2473e-01, -6.0977e-01,  1.9578e-01,\n",
      "         -4.0170e-01, -2.6625e-01, -1.7186e-01,  9.8320e-01, -4.5623e-01,\n",
      "         -5.6306e-02, -3.9281e-01, -7.5286e-01, -2.0356e-01, -2.9999e-01,\n",
      "          1.9472e-02,  6.4454e-01, -2.5124e-01, -3.2783e-01,  5.2403e-01,\n",
      "          7.8053e-01,  4.0195e-01, -4.4745e-02,  3.2456e-01, -3.9189e-01,\n",
      "          3.0384e-02, -2.8146e-01,  1.2777e-01,  3.5555e-01, -6.2894e-01,\n",
      "         -7.7203e-01, -2.0626e-01,  4.4717e-01, -7.0873e-01, -8.7486e-01,\n",
      "          2.8076e-02,  3.1094e-01,  2.3762e-01, -5.5104e-02,  1.2083e-01,\n",
      "          2.6305e-01,  4.7346e-01, -4.6563e-01, -3.4046e-01, -1.5508e-01,\n",
      "         -1.1613e-02, -3.8100e-01,  1.1898e-01,  9.9527e-01, -7.2460e-01,\n",
      "         -1.9933e-01,  7.9406e-01,  1.7134e-01,  8.1639e-01, -1.0085e-02,\n",
      "          3.3872e-01,  1.0073e-01, -4.9998e-01, -1.0727e-01, -4.3984e-01,\n",
      "          1.3953e-01,  4.5369e-02,  4.5635e-01, -9.7535e-01,  7.0649e-01,\n",
      "          1.2024e-01, -1.0116e-01, -3.2461e-01, -1.0964e-01, -9.2457e-01,\n",
      "          8.2265e-01,  2.0268e-01, -7.4479e-01, -6.9099e-01,  2.3879e-02,\n",
      "         -6.4643e-01,  3.3589e-01, -3.3130e-01, -5.1324e-01,  3.6819e-01,\n",
      "         -7.3821e-01, -5.7406e-02, -9.2591e-01,  2.2181e-01, -4.1155e-01,\n",
      "         -3.6628e-01, -9.1477e-01, -4.1140e-01, -8.5660e-01, -3.7052e-01,\n",
      "         -6.2514e-01, -5.9966e-02,  1.1590e-01, -2.5432e-01, -8.0376e-01,\n",
      "          2.8247e-01, -6.6585e-01,  2.9261e-01,  3.7384e-02,  5.1016e-01,\n",
      "          5.5746e-01,  5.5473e-01,  2.9618e-01,  5.4336e-01, -4.8593e-01,\n",
      "         -9.3465e-01, -2.2498e-01,  2.6892e-01, -3.4382e-01,  5.1971e-01,\n",
      "         -9.2402e-01,  1.4441e-01, -6.5392e-01,  4.2601e-01, -9.4127e-01,\n",
      "         -2.9307e-01, -7.1818e-03, -2.8322e-01,  7.8260e-01,  5.9386e-01,\n",
      "         -2.2292e-02,  1.0540e-01, -5.5862e-01,  2.1201e-01,  5.0886e-01,\n",
      "         -2.3087e-01, -1.5713e-01, -2.8031e-01,  6.2502e-01, -9.5168e-01,\n",
      "         -9.9855e-01, -2.8597e-01,  4.2114e-01,  7.4060e-02,  3.3080e-01,\n",
      "         -3.1853e-01, -2.7072e-01, -1.8844e-01,  7.3462e-01, -8.5185e-01,\n",
      "          6.0996e-01, -6.1674e-02,  8.0078e-01,  3.7814e-01, -3.5309e-01,\n",
      "          8.6977e-01, -2.6119e-01, -8.3250e-01,  2.8097e-01, -3.1958e-01,\n",
      "          1.6866e-01,  3.9469e-01, -2.2770e-01, -9.8586e-01,  2.1988e-01,\n",
      "          8.3379e-03,  6.2809e-01, -1.9986e-01]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
      "model.embeddings.position_ids\n",
      "torch.Size([1, 512])\n",
      "model.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "model.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "model.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "model.embeddings.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.embeddings.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.0.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.1.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.2.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.3.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.4.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.5.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.6.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.7.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.8.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.9.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "torch.Size([768])\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "torch.Size([768])\n",
      "model.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "model.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "embedding.weight\n",
      "torch.Size([19, 768])\n",
      "classifier_positive.0.weight\n",
      "torch.Size([50, 768])\n",
      "classifier_positive.0.bias\n",
      "torch.Size([50])\n",
      "classifier_positive.3.weight\n",
      "torch.Size([42, 50])\n",
      "classifier_positive.3.bias\n",
      "torch.Size([42])\n",
      "classifier_aux.0.weight\n",
      "torch.Size([50, 768])\n",
      "classifier_aux.0.bias\n",
      "torch.Size([50])\n",
      "classifier_aux.3.weight\n",
      "torch.Size([14, 50])\n",
      "classifier_aux.3.bias\n",
      "torch.Size([14])\n",
      "WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
      "u.shape\n",
      "(50,)\n",
      "contrastive_projection.shape\n",
      "(50, 50)\n",
      "encoded_orig.shape\n",
      "(1, 768)\n",
      "encoded.shape\n",
      "(66, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_723079/4019367649.py:109: RuntimeWarning: invalid value encountered in divide\n",
      "  contrastive_projection = np.outer(u, u) / np.dot(u, u)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 50 is different from 768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 121\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 121\u001b[0m z_all_row \u001b[38;5;241m=\u001b[39m \u001b[43mencoded_orig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontrastive_projection\u001b[49m\n\u001b[1;32m    122\u001b[0m z_h_row \u001b[38;5;241m=\u001b[39m encoded \u001b[38;5;241m@\u001b[39m contrastive_projection\n\u001b[1;32m    124\u001b[0m prediction_probabilities \u001b[38;5;241m=\u001b[39m softmax(z_all_row \u001b[38;5;241m@\u001b[39m classifier_w\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m classifier_b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 50 is different from 768)"
     ]
    }
   ],
   "source": [
    "all_interesting_results = []\n",
    "\n",
    "index2label = {0: \"not_claimed\", 1: \"claimed_and_violated\", 2: \"claimed_not_violated\"}\n",
    "\n",
    "for interesting_item in interesting_items: \n",
    "    out = interesting_item[\"out\"]\n",
    "    claims = interesting_item[\"claims\"]\n",
    "    ex = interesting_item[\"ex\"]\n",
    "    y = interesting_item[\"y\"]\n",
    "    facts = ex[\"facts\"]\n",
    "    gold = interesting_item[\"gold\"][0]\n",
    "    silver_rationales = interesting_item[\"silver_rationales\"]\n",
    "    \n",
    "    print(\"logits.shape\")\n",
    "    print(logits.shape)\n",
    "    print(\"out.shape\")\n",
    "    print(out.shape)\n",
    "    #encoded_orig, _ = preprocessing_for_bert([facts], tokenizer, max=512)\n",
    "\n",
    "    preprocessed_ex, preprocessed_ex_masks = preprocessing_for_bert([facts], tokenizer, max=512)\n",
    "    global_attention_mask = torch.zeros(preprocessed_ex.shape, dtype=torch.long, device=\"cuda\")\n",
    "    global_attention_mask[:, [0]] = 1\n",
    "    print(\"Q\"*100)\n",
    "    ou = model.model(preprocessed_ex.squeeze(1).cuda(), preprocessed_ex_masks.squeeze(1).cuda())\n",
    "    print(ou)\n",
    "    print(\"Q\"*100)\n",
    "    encoded_orig = model(preprocessed_ex.squeeze(1).cuda(), preprocessed_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), claims)[1]\n",
    "\n",
    "    #print('Predicted: ', facts)\n",
    "\n",
    "    facts_sentences = facts\n",
    "\n",
    "    masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "    masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "    encoded = []\n",
    "    mask_mapping = []\n",
    "    preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "    for m1_i, m1 in enumerate(masks1):\n",
    "        masked1 = []\n",
    "        for i in m1:\n",
    "            masked1[i] = '<mask>'\n",
    "        masked1 = ' '.join(masked1)\n",
    "        masked_sentence = []\n",
    "        for m2_i, m2 in enumerate(masks2):\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for i in m2:\n",
    "                masked_sentence.append(masked2[i])\n",
    "                sentence_length = len(tokenizer.tokenize(masked2[i]))\n",
    "                masked2[i] = '<mask> '*sentence_length\n",
    "            masked2 = tokenizer.tokenize(' '.join(masked2))\n",
    "                \n",
    "            masked_ex = {\n",
    "                \"facts\": masked2,\n",
    "                \"claims\": claims,\n",
    "                \"case_no\": ex['case_no']\n",
    "            }\n",
    "            \n",
    "            preprocessed_masked_ex, preprocessed_masked_ex_masks = preprocessing_for_bert([masked_ex[\"facts\"]], tokenizer, max=512)\n",
    "            global_attention_mask = torch.zeros(preprocessed_masked_ex.shape, dtype=torch.long, device=\"cuda\")\n",
    "            global_attention_mask[:, [0]] = 1\n",
    "            last_hidden_state_cls = model(preprocessed_masked_ex.squeeze(1).cuda(), preprocessed_masked_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), claims)[1]\n",
    "            #masked_out, last_hidden_state_cls = model(preprocessed_masked_ex.squeeze(1).cuda(), preprocessed_masked_ex_masks.squeeze(1).cuda(), global_attention_mask.squeeze(1), None)[0] #predictor.predict_json(masked_ex)\n",
    "\n",
    "            #print(\"indices\", m1_i, m2_i)\n",
    "            #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "            #print(\"gold labels\", masked_out['labels'])\n",
    "            #print(\"masked out sentence\", masked_sentence)\n",
    "            encoded.append(last_hidden_state_cls.cpu().detach())\n",
    "            mask_mapping.append((m1_i, m2_i))\n",
    "            \n",
    "            #print(\"====\")\n",
    "            \n",
    "    # make a tensor out of a list of tensors\n",
    "    encoded = torch.cat(encoded, dim=0)\n",
    "    encoded = np.array(encoded)\n",
    "\n",
    "    encoded_orig = np.array(encoded_orig.cpu().detach())\n",
    "\n",
    "    # replace some random f in the following list with another option from\n",
    "    # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "    label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    interesting_label_options = [\"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    article_id = random.choice([i for i in range(len(out[0])) if index2label[out[0][i].item()] in interesting_label_options or index2label[y[0][i].item()] in interesting_label_options])\n",
    "    foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(out[0])]\n",
    "\n",
    "    fact_idx = out[0][article_id].item()\n",
    "    foil_idx = label2index[foils[article_id]]\n",
    "    #print(\"article number\", articles[article_id])\n",
    "    #print('fact:', index2label[fact_idx])\n",
    "    #print('foil:', index2label[foil_idx])\n",
    "\n",
    "    fact_idx = article_id * len(label_options) + fact_idx\n",
    "    foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "    print(\"W\"*100)\n",
    "    for s in model_state_dict: \n",
    "        print(s)\n",
    "        print(model_state_dict[s].shape)\n",
    "    print(\"W\"*100)\n",
    "    classifier_w = model_state_dict[\"classifier_positive.3.weight\"].cpu().numpy()\n",
    "    classifier_b = model_state_dict[\"classifier_positive.3.bias\"].cpu().numpy()\n",
    "    #classifier_w = classifier_w.reshape(b_input_ids.shape[0], -1, 3)\n",
    "    #classifier_b = classifier_b.reshape(b_input_ids.shape[0], -1, 3)\n",
    "    #classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "    #classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "    u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "    contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "    print(\"u.shape\")\n",
    "    print(u.shape)\n",
    "    print(\"contrastive_projection.shape\")\n",
    "    print(contrastive_projection.shape)\n",
    "\n",
    "    z_all = encoded_orig \n",
    "    z_h = encoded \n",
    "    print(\"encoded_orig.shape\")\n",
    "    print(encoded_orig.shape)\n",
    "    print(\"encoded.shape\")\n",
    "    print(encoded.shape)\n",
    "    z_all_row = encoded_orig @ contrastive_projection\n",
    "    z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "    prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "    prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "    prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "    p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "    q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "    p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "    q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "    distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "    #print(\"the case\", ex['facts'])\n",
    "    #print(\"silver rationales\", silver_rationales)\n",
    "    #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "        \n",
    "    highlight_rankings = np.argsort(-distances)\n",
    "    explained_indices = []\n",
    "\n",
    "    for i in range(len(facts_sentences)):\n",
    "        rank = highlight_rankings[i]\n",
    "        m1_i, m2_i = mask_mapping[rank]\n",
    "        \n",
    "        masked_sentence = []\n",
    "        masked2 = facts_sentences.copy()\n",
    "        for k in masks2[m2_i]:\n",
    "            masked_sentence.append(masked2[k])\n",
    "            masked2[k] = '<mask>'\n",
    "        explained_indices.append(k)\n",
    "        masked2 = ' '.join(masked2)\n",
    "        #print(\"input with sentence masked out \\n\",masked2)\n",
    "        #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "        #print(\"omitted index\\n\", i)\n",
    "        #print(np.round(distances[rank], 4))\n",
    "        \n",
    "    #print(explained_indices)\n",
    "    all_interesting_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "    print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "985\n",
      "meanPrecision@ 2   0.9022988505747126\n",
      "meanRecall@ 2   0.14178598326617356\n",
      "meanPrecision@ 3   0.7892720306513408\n",
      "meanRecall@ 3   0.1763480453128247\n",
      "meanPrecision@ 4   0.7155172413793104\n",
      "meanRecall@ 4   0.2056083138118052\n",
      "meanPrecision@ 5   0.6505747126436783\n",
      "meanRecall@ 5   0.22842611003506907\n",
      "meanPrecision@ 6   0.6034482758620688\n",
      "meanRecall@ 6   0.24904954889156178\n",
      "meanPrecision@ 7   0.5533661740558292\n",
      "meanRecall@ 7   0.2615054696612314\n",
      "meanPrecision@ 8   0.5100574712643678\n",
      "meanRecall@ 8   0.271469390094073\n",
      "meanPrecision@ 9   0.47126436781609204\n",
      "meanRecall@ 9   0.2781305787759071\n"
     ]
    }
   ],
   "source": [
    "print(len(all_interesting_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_interesting_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_interesting_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "985\n",
      "meanPrecision@ 2   nan\n",
      "meanRecall@ 2   nan\n",
      "meanPrecision@ 3   nan\n",
      "meanRecall@ 3   nan\n",
      "meanPrecision@ 4   nan\n",
      "meanRecall@ 4   nan\n",
      "meanPrecision@ 5   nan\n",
      "meanRecall@ 5   nan\n",
      "meanPrecision@ 6   nan\n",
      "meanRecall@ 6   nan\n",
      "meanPrecision@ 7   nan\n",
      "meanRecall@ 7   nan\n",
      "meanPrecision@ 8   nan\n",
      "meanRecall@ 8   nan\n",
      "meanPrecision@ 9   nan\n",
      "meanRecall@ 9   nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         incorrect_items\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m:out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m:claims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m:outcomes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\"\u001b[39m:ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold\u001b[39m\u001b[38;5;124m\"\u001b[39m:gold, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_rationales\u001b[39m\u001b[38;5;124m\"\u001b[39m:silver_rationales})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# save incorrect items\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../experiments/ecthr/incorrect_items_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m incorrect_items:\n\u001b[1;32m     18\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(item) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'"
     ]
    }
   ],
   "source": [
    "incorrect_items = []\n",
    "non_zero = 0\n",
    "for e in val_data: \n",
    "    preprocessed_e = preprocessing_for_bert([e[\"facts_sentences\"]], tokenizer, max=512)\n",
    "    out, last_hidden_state_cls = model(preprocessed_e)[0] #predictor.predict_json(e)\n",
    "    claims = e[\"claims\"]\n",
    "    gold = [\"not_claimed\" if c == 0 else \"claimed_not_violated\" if c == 1 and o == 0 else \"claimed_and_violated\" for c, o in zip(claims, outcomes)]\n",
    "    gold_id = e[\"case_no\"]\n",
    "    silver_rationales = [i for i in val_meta_data if i[\"case_no\"] == gold_id][0][\"silver_rationales\"]\n",
    "    if out[\"labels\"] != gold and silver_rationales:\n",
    "        non_zero += 1\n",
    "        ex = e\n",
    "        incorrect_items.append({\"out\":out, \"claims\":claims, \"ex\":ex, \"gold\":gold, \"silver_rationales\":silver_rationales})\n",
    "\n",
    "# save items from the incorrect_items list to a file\n",
    "with open(f\"./incorrect_items.txt\", \"w\") as f:\n",
    "    for item in incorrect_items:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# if the file f\"./incorrect_results.txt\" exists then remove it\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    os.remove(f\"./incorrect_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "985\n",
      "meanPrecision@ 2   0.9477739726027398\n",
      "meanRecall@ 2   0.09767005583962775\n",
      "meanPrecision@ 3   0.8978310502283103\n",
      "meanRecall@ 3   0.135352180169421\n",
      "meanPrecision@ 4   0.8441780821917808\n",
      "meanRecall@ 4   0.16501159742012056\n",
      "meanPrecision@ 5   0.7948630136986301\n",
      "meanRecall@ 5   0.1890894937220949\n",
      "meanPrecision@ 6   0.7471461187214612\n",
      "meanRecall@ 6   0.2077014659264444\n",
      "meanPrecision@ 7   0.7045009784735813\n",
      "meanRecall@ 7   0.2227548321363115\n",
      "meanPrecision@ 8   0.6673801369863014\n",
      "meanRecall@ 8   0.23588981179558793\n",
      "meanPrecision@ 9   0.6322298325722984\n",
      "meanRecall@ 9   0.2463872776379505\n"
     ]
    }
   ],
   "source": [
    "# read in incorrect items\n",
    "with open(f\"./incorrect_items.txt\", \"r\") as f:\n",
    "    incorrect_items = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "#check if a file in the path ./incorrect_results.txt exists\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    # load incorrect results from the incorrect_results file\n",
    "    with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "        incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "else: \n",
    "    incorrect_results = []\n",
    "    \n",
    "saved_exes = [i[\"ex\"] for i in incorrect_results]\n",
    "\n",
    "all_incorrect_results = []\n",
    "\n",
    "for incorrect_item in incorrect_items: \n",
    "    out = incorrect_item[\"out\"]\n",
    "    claims = incorrect_item[\"claims\"]\n",
    "    outcomes = incorrect_item[\"outcomes\"]\n",
    "    ex = incorrect_item[\"ex\"]\n",
    "    gold = incorrect_item[\"gold\"]\n",
    "    facts = ex[\"facts\"]\n",
    "    silver_rationales = incorrect_item[\"silver_rationales\"]\n",
    "    \n",
    "    if ex not in saved_exes:\n",
    "\n",
    "        encoded_orig = out['encoded_representations']\n",
    "\n",
    "        facts = out['labels']\n",
    "        #print('Predicted: ', facts)\n",
    "\n",
    "        facts_sentences = facts\n",
    "\n",
    "        masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "        masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "        encoded = []\n",
    "        mask_mapping = []\n",
    "        preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "        for m1_i, m1 in enumerate(masks1):\n",
    "            masked1 = []\n",
    "            for i in m1:\n",
    "                masked1[i] = '<mask>'\n",
    "            masked1 = ' '.join(masked1)\n",
    "            masked_sentence = []\n",
    "            for m2_i, m2 in enumerate(masks2):\n",
    "                masked2 = facts_sentences.copy()\n",
    "                for i in m2:\n",
    "                    masked_sentence.append(masked2[i])\n",
    "                    sentence_length = len(tokenizer.tokenize(masked2[i]))\n",
    "                    masked2[i] = '<mask> '*sentence_length\n",
    "                masked2 = tokenizer.tokenize(' '.join(masked2))\n",
    "                    \n",
    "                masked_ex = {\n",
    "                    \"facts\": masked2,\n",
    "                    \"claims\": claims,\n",
    "                    \"outcomes\": outcomes,\n",
    "                    \"case_no\": ex['case_no']\n",
    "                }\n",
    "                \n",
    "                preprocessed_masked_ex = preprocessing_for_bert([masked_ex[\"facts\"]], tokenizer, max=512)\n",
    "                masked_out, last_hidden_state_cls = model(preprocessed_masked_ex)[0] #predictor.predict_json(masked_ex)\n",
    "\n",
    "                #print(\"indices\", m1_i, m2_i)\n",
    "                #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "                #print(\"gold labels\", masked_out['labels'])\n",
    "                #print(\"masked out sentence\", masked_sentence)\n",
    "                encoded.append(last_hidden_state_cls)\n",
    "                mask_mapping.append((m1_i, m2_i))\n",
    "                \n",
    "                #print(\"====\")\n",
    "            \n",
    "        encoded = np.array(encoded)\n",
    "\n",
    "        # replace some random f in the following list with another option from\n",
    "        # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "        label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "        article_id = random.choice([i for i in range(len(facts))])\n",
    "        foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(facts)]\n",
    "\n",
    "        fact_idx = label2index[facts[article_id]]\n",
    "        foil_idx = label2index[foils[article_id]]\n",
    "        #print(\"article number\", articles[article_id])\n",
    "        #print('fact:', index2label[fact_idx])\n",
    "        #print('foil:', index2label[foil_idx])\n",
    "\n",
    "        fact_idx = article_id * len(label_options) + fact_idx\n",
    "        foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "        classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "        classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "        u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "        contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "        z_all = encoded_orig \n",
    "        z_h = encoded \n",
    "        z_all_row = encoded_orig @ contrastive_projection\n",
    "        z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "        prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "        prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "        prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "        p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "        q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "        p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "        q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "        distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "        #print(\"the case\", ex['facts'])\n",
    "        #print(\"silver rationales\", silver_rationales)\n",
    "        #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "            \n",
    "        highlight_rankings = np.argsort(-distances)\n",
    "        explained_indices = []\n",
    "\n",
    "        for i in range(len(facts_sentences)):\n",
    "            rank = highlight_rankings[i]\n",
    "            m1_i, m2_i = mask_mapping[rank]\n",
    "            \n",
    "            masked_sentence = []\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for k in masks2[m2_i]:\n",
    "                masked_sentence.append(masked2[k])\n",
    "                masked2[k] = '<mask>'\n",
    "            explained_indices.append(k)\n",
    "            masked2 = ' '.join(masked2)\n",
    "            #print(\"input with sentence masked out \\n\",masked2)\n",
    "            #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "            #print(\"omitted index\\n\", i)\n",
    "            #print(np.round(distances[rank], 4))\n",
    "            \n",
    "        #print(explained_indices)\n",
    "        all_incorrect_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        # append incorrect result to an external incorrect_results file\n",
    "        with open(f\"./incorrect_results.txt\", \"a\") as f:\n",
    "            f.write(json.dumps({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices}) + \"\\n\")\n",
    "\n",
    "# read in all_incorrect_results from the incorrect_results file\n",
    "with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "    all_incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/simple_val.jsonl\", \"r\") as f:\n",
    "    val_data = [json.loads(line) for line in f.readlines()]       \n",
    "\n",
    "print(len(all_incorrect_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_incorrect_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_incorrect_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
