{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/irs38/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel, AutoModel, LongformerModel\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from scipy.special import softmax\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"nlpaueb/legal-bert-base-uncased\"\n",
    "#model = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "model_path = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/results/Outcome/joint_model/legal_bert/facts/adbb938c3853499b8abe717122c4e05e/model.pt\"\n",
    "model = torch.load(model_path)\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "#archive = load_archive(model_path + '/model.tar.gz')\n",
    "#print(archive.config)\n",
    "#archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "#archive.config['model']['output_hidden_states'] = True\n",
    "#model = archive.model\n",
    "#model._output_hidden_states = True\n",
    "#predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data, tokenizer, max=512):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        sent = \" \".join(sent)\n",
    "        sent = sent[:500000] # Speeds the process up for documents with a lot of precedent we would truncate anyway.\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            # return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append([encoded_sent.get('input_ids')])\n",
    "        attention_masks.append([encoded_sent.get('attention_mask')])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "#    label2index = json.load(f)\n",
    "#    index2label = {label2index[k]: k for k in label2index}\n",
    "#label2index\n",
    "# [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "\n",
    "label2index = {\"not_claimed\":0, \"claimed_and_violated\":1, \"claimed_not_violated\":2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "\n",
    "def precisionAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recallAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def meanPrecisionAtK(actual, predicted, k):\n",
    "    return np.mean([precisionAtK(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def meanRecallAtK(actual, predicted, k):\n",
    "    return np.mean([recallAtK(a, p, k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m val_data: \n\u001b[1;32m     24\u001b[0m     e_facts, e_masks \u001b[38;5;241m=\u001b[39m preprocessing_for_bert([e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacts_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m]], tokenizer, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_facts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#predictor.predict_json(e)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     claims \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     outcomes \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/contrastive-explanations/notebooks/model.py:98\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention, claims)\u001b[0m\n\u001b[1;32m     96\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, global_attention_mask\u001b[38;5;241m=\u001b[39mglobal_attention)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Extract the last hidden state of the token `[CLS]` for classification task\u001b[39;00m\n\u001b[1;32m    101\u001b[0m last_hidden_state_cls \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:837\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 837\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    841\u001b[0m     embedding_output,\n\u001b[1;32m    842\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    849\u001b[0m )\n\u001b[1;32m    850\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:197\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    194\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m    199\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/sparse.py:124\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/functional.py:1814\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m#   torch.nembedding_renorm_\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 1814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "#read in the validation data, which is a json dict in each new line\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/simple_val.jsonl\", \"r\") as f:\n",
    "    val_data = [json.loads(line) for line in f.readlines()]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/dev.jsonl\", \"r\") as f:\n",
    "    val_meta_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for item in val_data: \n",
    "    item[\"facts_sentences\"] = [i for i in val_meta_data if i[\"case_no\"] == item[\"case_no\"]][0][\"facts\"]\n",
    "\n",
    "articles = ['10', '11', '13', '14', '18', '2', '3', '4', '5', '6', '7', '8', '9', 'P1-1', 'P4-2', 'P7-1', 'P7-4']\n",
    "\n",
    "#ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "#ex = {\"facts\": \"4.  The applicant was born in 1960 and lives in Oleksandrivka, Kirovograd Region. 5.  On 3 February 2007 the applicant was assaulted. According to the subsequent findings of medical experts, she sustained haematomas on her jaw, shoulder and hip, a bruise under her right eye, concussion, and a displaced rib fracture. The applicant alleges that her assailants were Mr and Mrs K., her daughter\\u2019s former parents-in-law, whereas the domestic authorities found that it was only Mrs K. who had assaulted the applicant. The incident occurred in front of the applicant\\u2019s two-year-old granddaughter. 6.  On 4 February 2007 the applicant lodged a complaint with the police. 7.  On 5 February 2007 a forensic medical expert examined the applicant. He found that she had haematomas which he classified as \\u201cminor bodily injuries\\u201d. 8.  On 14 February 2007 the Oleksandrivka District Police Department (\\u201cthe Oleksandrivka police\\u201d) refused to institute criminal proceedings in connection with the incident. 9.  On 22 February 2007 a forensic medical examination of the applicant was carried out. The expert found that in addition to the previously noted haematomas, the applicant had also suffered concussion and a displaced rib fracture. The expert classified the injuries as \\u201cbodily harm of medium severity\\u201d. 10.  On 20 March 2007 the Oleksandrivka prosecutor overruled the decision of 14 February 2007 as premature and on 21 March 2007 instituted criminal proceedings in connection with the infliction of bodily harm of medium severity on the applicant. 11.  On 20 May 2007 the investigator suspended the investigation for failure to identify the perpetrator. 12.  On 29 August and 3 October 2007 the Oleksandrivka prosecutor\\u2019s office issued two decisions in which it overruled the investigator\\u2019s decision of 20 May 2007 as premature. 13.  On 6 October 2007 the investigator questioned Mr and Mrs K. 14.  On 1 December 2007 the investigator again suspended the investigation for failure to identify the perpetrator. 15.  On 10 December 2007 the Oleksandrivka prosecutor\\u2019s office, in response to the applicant\\u2019s complaint about the progress of the investigation, asked the Kirovograd Regional Police Department to have the police officers in charge of the investigation disciplined. 16.  On 21 January 2008 the Kirovograd Regional Police Department instructed the Oleksandrivka police to immediately resume the investigation. 17.  On 7 April 2008 the investigator decided to ask a forensic medical expert to determine the degree of gravity of the applicant\\u2019s injuries. On 22 September 2008 the expert drew up a report generally confirming the findings of 22 February 2007. 18.  On 15 May 2008 the Kirovograd Regional Police Department informed the applicant that the police officers in charge of the case had been disciplined for omissions in the investigation. 19.  On 23 October 2008 the Oleksandrivka Court absolved Mrs K. from criminal liability under an amnesty law, on the grounds that she had an elderly mother who was dependent on her. On 24 February 2009 the Kirovograd Regional Court of Appeal (\\u201cthe Court of Appeal\\u201d) quashed that judgment, finding no evidence that Mrs K.\\u2019s mother was dependent on her. 20.  On 1 July 2009 the investigator refused to institute criminal proceedings against Mr K. 21.  On 7 July 2009 the Novomyrgorod prosecutor issued a bill of indictment against Mrs K. 22.  On 24 July 2009 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding that the applicant had not been informed about the completion of the investigation until 3 July 2009 and had therefore not been given enough time to study the case file. It also held that the refusal to institute criminal proceedings against Mr K. had contravened the law. 23.  On 13 November 2009 the Novomyrgorod prosecutor quashed the decision of 1 July 2009 not to institute criminal proceedings against Mr K. Subsequently the investigator again refused to institute criminal proceedings against Mr K. 24.  On 21 December 2009 the new round of pre-trial investigation in the case against Mrs K. was completed and another bill of indictment was issued by the Novomyrgorod prosecutor. 25.  On 29 March 2010 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding in particular that the decision not to institute criminal proceedings against Mr K. had been premature, since his role in the incident had not been sufficiently clarified. 26.  On 13 July 2010 the Novomyrgorod prosecutor quashed the decision not to institute criminal proceedings against Mr K. On 26 May 2011 the investigator again refused to institute criminal proceedings against Mr K. 27.  On 20 December 2011 the Znamyanka Court convicted Mrs K. of inflicting bodily harm of medium severity on the applicant, sentencing her to restriction of liberty for two years, suspended for a one-year probationary period. The court found that the decision not to institute criminal proceedings against Mr K. in connection with the same incident had been correct. Mrs K., the prosecutor and the applicant appealed. 28.  On 6 March 2012 the Court of Appeal quashed the judgment and discontinued the criminal proceedings against Mrs K. as time-barred.\", \"claims\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"27454/11\"}\n",
    "\n",
    "#shuffle val_data\n",
    "random.shuffle(val_data)\n",
    "\n",
    "interesting_items = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "non_zero = 0\n",
    "for e in val_data: \n",
    "    e_facts, e_masks = preprocessing_for_bert([e[\"facts_sentences\"]], tokenizer, max=512)\n",
    "    out = model(e_facts, e_masks, None, None)[0] #predictor.predict_json(e)\n",
    "    claims = e[\"claims\"]\n",
    "    outcomes = e[\"outcomes\"]\n",
    "    gold = [\"not_claimed\" if c == 0 else \"claimed_not_violated\" if c == 1 and o == 0 else \"claimed_and_violated\" for c, o in zip(claims, outcomes)]\n",
    "    gold_id = e[\"case_no\"]\n",
    "    silver_rationales = [i for i in val_meta_data if i[\"case_no\"] == gold_id][0][\"silver_rationales\"]\n",
    "    if len(set(out[\"labels\"])) != 1 and out[\"labels\"] == gold and silver_rationales:\n",
    "        #print(out[\"labels\"])\n",
    "        non_zero += 1\n",
    "        ex = e\n",
    "        interesting_items.append({\"out\":out, \"claims\":claims, \"outcomes\":outcomes, \"ex\":ex, \"gold\":gold, \"silver_rationales\":silver_rationales})\n",
    "        #break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interesting_items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m all_interesting_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interesting_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43minteresting_items\u001b[49m: \n\u001b[1;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m interesting_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     claims \u001b[38;5;241m=\u001b[39m interesting_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interesting_items' is not defined"
     ]
    }
   ],
   "source": [
    "all_interesting_results = []\n",
    "\n",
    "for interesting_item in interesting_items: \n",
    "    out = interesting_item[\"out\"]\n",
    "    claims = interesting_item[\"claims\"]\n",
    "    outcomes = interesting_item[\"outcomes\"]\n",
    "    ex = interesting_item[\"ex\"]\n",
    "    gold = interesting_item[\"gold\"]\n",
    "    silver_rationales = interesting_item[\"silver_rationales\"]\n",
    "    \n",
    "    encoded_orig = out['encoded_representations']\n",
    "\n",
    "    facts = out['labels']\n",
    "    #print('Predicted: ', facts)\n",
    "\n",
    "    tokenizer.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "\n",
    "    facts_sentences = ex[\"facts_sentences\"]\n",
    "\n",
    "    masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "    masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "    encoded = []\n",
    "    mask_mapping = []\n",
    "    preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "    for m1_i, m1 in enumerate(masks1):\n",
    "        masked1 = []\n",
    "        for i in m1:\n",
    "            masked1[i] = '<mask>'\n",
    "        masked1 = ' '.join(masked1)\n",
    "        masked_sentence = []\n",
    "        for m2_i, m2 in enumerate(masks2):\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for i in m2:\n",
    "                masked_sentence.append(masked2[i])\n",
    "                sentence_length = len(tokenizer.tokenize(masked2[i]))\n",
    "                masked2[i] = '<mask> '*sentence_length\n",
    "            masked2 = tokenizer.tokenize(' '.join(masked2))\n",
    "                \n",
    "            masked_ex = {\n",
    "                \"facts\": masked2,\n",
    "                \"claims\": claims,\n",
    "                \"outcomes\": outcomes,\n",
    "                \"case_no\": ex['case_no']\n",
    "            }\n",
    "            \n",
    "            preprocessed_masked_ex = preprocessing_for_bert([masked_ex[\"facts\"]], tokenizer, max=512)\n",
    "            masked_out = model(preprocessed_masked_ex)[0] #predictor.predict_json(masked_ex)\n",
    "\n",
    "            #print(\"indices\", m1_i, m2_i)\n",
    "            #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "            #print(\"gold labels\", masked_out['labels'])\n",
    "            #print(\"masked out sentence\", masked_sentence)\n",
    "            encoded.append(masked_out['encoded_representations'])\n",
    "            mask_mapping.append((m1_i, m2_i))\n",
    "            \n",
    "            #print(\"====\")\n",
    "            \n",
    "    encoded = np.array(encoded)\n",
    "\n",
    "    # replace some random f in the following list with another option from\n",
    "    # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "    label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    interesting_label_options = [\"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    article_id = random.choice([i for i in range(len(facts)) if facts[i] in interesting_label_options or gold[i] in interesting_label_options])\n",
    "    foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(facts)]\n",
    "\n",
    "    fact_idx = label2index[facts[article_id]]\n",
    "    foil_idx = label2index[foils[article_id]]\n",
    "    #print(\"article number\", articles[article_id])\n",
    "    #print('fact:', index2label[fact_idx])\n",
    "    #print('foil:', index2label[foil_idx])\n",
    "\n",
    "    fact_idx = article_id * len(label_options) + fact_idx\n",
    "    foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "    classifier_w = model_state_dict[\"classifier.3.weight\"].numpy()\n",
    "    classifier_b = model_state_dict[\"classifier.3.bias\"].numpy()\n",
    "    #classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "    #classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "    u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "    contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "    #print(contrastive_projection.shape)\n",
    "\n",
    "    z_all = encoded_orig \n",
    "    z_h = encoded \n",
    "    z_all_row = encoded_orig @ contrastive_projection\n",
    "    z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "    prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "    prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "    prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "    p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "    q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "    p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "    q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "    distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "    #print(\"the case\", ex['facts'])\n",
    "    #print(\"silver rationales\", silver_rationales)\n",
    "    #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "        \n",
    "    highlight_rankings = np.argsort(-distances)\n",
    "    explained_indices = []\n",
    "\n",
    "    for i in range(len(facts_sentences)):\n",
    "        rank = highlight_rankings[i]\n",
    "        m1_i, m2_i = mask_mapping[rank]\n",
    "        \n",
    "        masked_sentence = []\n",
    "        masked2 = facts_sentences.copy()\n",
    "        for k in masks2[m2_i]:\n",
    "            masked_sentence.append(masked2[k])\n",
    "            masked2[k] = '<mask>'\n",
    "        explained_indices.append(k)\n",
    "        masked2 = ' '.join(masked2)\n",
    "        #print(\"input with sentence masked out \\n\",masked2)\n",
    "        #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "        #print(\"omitted index\\n\", i)\n",
    "        #print(np.round(distances[rank], 4))\n",
    "        \n",
    "    #print(explained_indices)\n",
    "    all_interesting_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "    print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "985\n",
      "meanPrecision@ 2   0.9022988505747126\n",
      "meanRecall@ 2   0.14178598326617356\n",
      "meanPrecision@ 3   0.7892720306513408\n",
      "meanRecall@ 3   0.1763480453128247\n",
      "meanPrecision@ 4   0.7155172413793104\n",
      "meanRecall@ 4   0.2056083138118052\n",
      "meanPrecision@ 5   0.6505747126436783\n",
      "meanRecall@ 5   0.22842611003506907\n",
      "meanPrecision@ 6   0.6034482758620688\n",
      "meanRecall@ 6   0.24904954889156178\n",
      "meanPrecision@ 7   0.5533661740558292\n",
      "meanRecall@ 7   0.2615054696612314\n",
      "meanPrecision@ 8   0.5100574712643678\n",
      "meanRecall@ 8   0.271469390094073\n",
      "meanPrecision@ 9   0.47126436781609204\n",
      "meanRecall@ 9   0.2781305787759071\n"
     ]
    }
   ],
   "source": [
    "print(len(all_interesting_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_interesting_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_interesting_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "985\n",
      "meanPrecision@ 2   nan\n",
      "meanRecall@ 2   nan\n",
      "meanPrecision@ 3   nan\n",
      "meanRecall@ 3   nan\n",
      "meanPrecision@ 4   nan\n",
      "meanRecall@ 4   nan\n",
      "meanPrecision@ 5   nan\n",
      "meanRecall@ 5   nan\n",
      "meanPrecision@ 6   nan\n",
      "meanRecall@ 6   nan\n",
      "meanPrecision@ 7   nan\n",
      "meanRecall@ 7   nan\n",
      "meanPrecision@ 8   nan\n",
      "meanRecall@ 8   nan\n",
      "meanPrecision@ 9   nan\n",
      "meanRecall@ 9   nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         incorrect_items\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m:out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m:claims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m:outcomes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\"\u001b[39m:ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold\u001b[39m\u001b[38;5;124m\"\u001b[39m:gold, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_rationales\u001b[39m\u001b[38;5;124m\"\u001b[39m:silver_rationales})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# save incorrect items\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../experiments/ecthr/incorrect_items_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m incorrect_items:\n\u001b[1;32m     18\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(item) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'"
     ]
    }
   ],
   "source": [
    "incorrect_items = []\n",
    "non_zero = 0\n",
    "for e in val_data: \n",
    "    preprocessed_e = preprocessing_for_bert([e[\"facts_sentences\"]], tokenizer, max=512)\n",
    "    out = model(preprocessed_e)[0] #predictor.predict_json(e)\n",
    "    claims = e[\"claims\"]\n",
    "    outcomes = e[\"outcomes\"]\n",
    "    gold = [\"not_claimed\" if c == 0 else \"claimed_not_violated\" if c == 1 and o == 0 else \"claimed_and_violated\" for c, o in zip(claims, outcomes)]\n",
    "    gold_id = e[\"case_no\"]\n",
    "    silver_rationales = [i for i in val_meta_data if i[\"case_no\"] == gold_id][0][\"silver_rationales\"]\n",
    "    if out[\"labels\"] != gold and silver_rationales:\n",
    "        non_zero += 1\n",
    "        ex = e\n",
    "        incorrect_items.append({\"out\":out, \"claims\":claims, \"outcomes\":outcomes, \"ex\":ex, \"gold\":gold, \"silver_rationales\":silver_rationales})\n",
    "\n",
    "# save items from the incorrect_items list to a file\n",
    "with open(f\"./incorrect_items.txt\", \"w\") as f:\n",
    "    for item in incorrect_items:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# if the file f\"./incorrect_results.txt\" exists then remove it\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    os.remove(f\"./incorrect_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "985\n",
      "meanPrecision@ 2   0.9477739726027398\n",
      "meanRecall@ 2   0.09767005583962775\n",
      "meanPrecision@ 3   0.8978310502283103\n",
      "meanRecall@ 3   0.135352180169421\n",
      "meanPrecision@ 4   0.8441780821917808\n",
      "meanRecall@ 4   0.16501159742012056\n",
      "meanPrecision@ 5   0.7948630136986301\n",
      "meanRecall@ 5   0.1890894937220949\n",
      "meanPrecision@ 6   0.7471461187214612\n",
      "meanRecall@ 6   0.2077014659264444\n",
      "meanPrecision@ 7   0.7045009784735813\n",
      "meanRecall@ 7   0.2227548321363115\n",
      "meanPrecision@ 8   0.6673801369863014\n",
      "meanRecall@ 8   0.23588981179558793\n",
      "meanPrecision@ 9   0.6322298325722984\n",
      "meanRecall@ 9   0.2463872776379505\n"
     ]
    }
   ],
   "source": [
    "# read in incorrect items\n",
    "with open(f\"./incorrect_items.txt\", \"r\") as f:\n",
    "    incorrect_items = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "#check if a file in the path ./incorrect_results.txt exists\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    # load incorrect results from the incorrect_results file\n",
    "    with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "        incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "else: \n",
    "    incorrect_results = []\n",
    "    \n",
    "saved_exes = [i[\"ex\"] for i in incorrect_results]\n",
    "\n",
    "all_incorrect_results = []\n",
    "\n",
    "for incorrect_item in incorrect_items: \n",
    "    out = incorrect_item[\"out\"]\n",
    "    claims = incorrect_item[\"claims\"]\n",
    "    outcomes = incorrect_item[\"outcomes\"]\n",
    "    ex = incorrect_item[\"ex\"]\n",
    "    gold = incorrect_item[\"gold\"]\n",
    "    silver_rationales = incorrect_item[\"silver_rationales\"]\n",
    "    \n",
    "    if ex not in saved_exes:\n",
    "\n",
    "        encoded_orig = out['encoded_representations']\n",
    "\n",
    "        facts = out['labels']\n",
    "        #print('Predicted: ', facts)\n",
    "\n",
    "        tokenizer.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "\n",
    "        facts_sentences = ex[\"facts_sentences\"]\n",
    "\n",
    "        masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "        masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "        encoded = []\n",
    "        mask_mapping = []\n",
    "        preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "        for m1_i, m1 in enumerate(masks1):\n",
    "            masked1 = []\n",
    "            for i in m1:\n",
    "                masked1[i] = '<mask>'\n",
    "            masked1 = ' '.join(masked1)\n",
    "            masked_sentence = []\n",
    "            for m2_i, m2 in enumerate(masks2):\n",
    "                masked2 = facts_sentences.copy()\n",
    "                for i in m2:\n",
    "                    masked_sentence.append(masked2[i])\n",
    "                    sentence_length = len(tokenizer.tokenize(masked2[i]))\n",
    "                    masked2[i] = '<mask> '*sentence_length\n",
    "                masked2 = tokenizer.tokenize(' '.join(masked2))\n",
    "                    \n",
    "                masked_ex = {\n",
    "                    \"facts\": masked2,\n",
    "                    \"claims\": claims,\n",
    "                    \"outcomes\": outcomes,\n",
    "                    \"case_no\": ex['case_no']\n",
    "                }\n",
    "                \n",
    "                preprocessed_masked_ex = preprocessing_for_bert([masked_ex[\"facts\"]], tokenizer, max=512)\n",
    "                masked_out = model(preprocessed_masked_ex)[0] #predictor.predict_json(masked_ex)\n",
    "\n",
    "                #print(\"indices\", m1_i, m2_i)\n",
    "                #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "                #print(\"gold labels\", masked_out['labels'])\n",
    "                #print(\"masked out sentence\", masked_sentence)\n",
    "                encoded.append(masked_out['encoded_representations'])\n",
    "                mask_mapping.append((m1_i, m2_i))\n",
    "                \n",
    "                #print(\"====\")\n",
    "            \n",
    "        encoded = np.array(encoded)\n",
    "\n",
    "        # replace some random f in the following list with another option from\n",
    "        # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "        label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "        article_id = random.choice([i for i in range(len(facts))])\n",
    "        foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(facts)]\n",
    "\n",
    "        fact_idx = label2index[facts[article_id]]\n",
    "        foil_idx = label2index[foils[article_id]]\n",
    "        #print(\"article number\", articles[article_id])\n",
    "        #print('fact:', index2label[fact_idx])\n",
    "        #print('foil:', index2label[foil_idx])\n",
    "\n",
    "        fact_idx = article_id * len(label_options) + fact_idx\n",
    "        foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "        classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "        classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "        u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "        contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "        #print(contrastive_projection.shape)\n",
    "\n",
    "        z_all = encoded_orig \n",
    "        z_h = encoded \n",
    "        z_all_row = encoded_orig @ contrastive_projection\n",
    "        z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "        prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "        prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "        prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "        p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "        q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "        p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "        q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "        distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "        #print(\"the case\", ex['facts'])\n",
    "        #print(\"silver rationales\", silver_rationales)\n",
    "        #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "            \n",
    "        highlight_rankings = np.argsort(-distances)\n",
    "        explained_indices = []\n",
    "\n",
    "        for i in range(len(facts_sentences)):\n",
    "            rank = highlight_rankings[i]\n",
    "            m1_i, m2_i = mask_mapping[rank]\n",
    "            \n",
    "            masked_sentence = []\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for k in masks2[m2_i]:\n",
    "                masked_sentence.append(masked2[k])\n",
    "                masked2[k] = '<mask>'\n",
    "            explained_indices.append(k)\n",
    "            masked2 = ' '.join(masked2)\n",
    "            #print(\"input with sentence masked out \\n\",masked2)\n",
    "            #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "            #print(\"omitted index\\n\", i)\n",
    "            #print(np.round(distances[rank], 4))\n",
    "            \n",
    "        #print(explained_indices)\n",
    "        all_incorrect_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        # append incorrect result to an external incorrect_results file\n",
    "        with open(f\"./incorrect_results.txt\", \"a\") as f:\n",
    "            f.write(json.dumps({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices}) + \"\\n\")\n",
    "\n",
    "# read in all_incorrect_results from the incorrect_results file\n",
    "with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "    all_incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/simple_val.jsonl\", \"r\") as f:\n",
    "    val_data = [json.loads(line) for line in f.readlines()]       \n",
    "\n",
    "print(len(all_incorrect_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_incorrect_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_incorrect_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
