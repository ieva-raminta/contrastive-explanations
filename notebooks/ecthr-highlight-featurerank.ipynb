{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file ../experiments/models/ecthr/allenai/longformer-base-4096/model.tar.gz not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m MODEL_NAME\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallenai/longformer-base-4096\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m model_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../experiments/models/\u001b[39m\u001b[39m{\u001b[39;00mDATASET\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mMODEL_NAME\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m archive \u001b[39m=\u001b[39m load_archive(model_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/model.tar.gz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(archive\u001b[39m.\u001b[39mconfig)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-gpu-irs38.cl.cam.ac.uk/home/irs38/contrastive-explanations/notebooks/ecthr-highlight-featurerank.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m archive\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mdataset_reader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mecthr\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/models/archival.py:154\u001b[0m, in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mInstantiates an Archive from an archived `tar.gz` file.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m    The weights file to use.  If unspecified, weights.th in the archive_file will be used.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m# redirect to the cache, if necessary\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m resolved_archive_file \u001b[39m=\u001b[39m cached_path(archive_file)\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39m==\u001b[39m archive_file:\n\u001b[1;32m    157\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading archive file \u001b[39m\u001b[39m{\u001b[39;00marchive_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/common/file_utils.py:202\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, extract_archive, force_extract)\u001b[0m\n\u001b[1;32m    198\u001b[0m         extraction_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_dir, extraction_name)\n\u001b[1;32m    200\u001b[0m \u001b[39melif\u001b[39;00m parsed\u001b[39m.\u001b[39mscheme \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# File, but it doesn't exist.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile \u001b[39m\u001b[39m{\u001b[39;00murl_or_filename\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[39m# Something unknown\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munable to parse \u001b[39m\u001b[39m{\u001b[39;00murl_or_filename\u001b[39m}\u001b[39;00m\u001b[39m as a URL or as a local path\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: file ../experiments/models/ecthr/allenai/longformer-base-4096/model.tar.gz not found"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"allenai/longformer-base-4096\"\n",
    "model_path=f\"../experiments/models/{DATASET}/{MODEL_NAME}\"\n",
    "\n",
    "archive = load_archive(model_path + '/model.tar.gz')\n",
    "print(archive.config)\n",
    "archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "archive.config['model']['output_hidden_states'] = True\n",
    "model = archive.model\n",
    "model._output_hidden_states = True\n",
    "predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "    label2index = json.load(f)\n",
    "    index2label = {label2index[k]: k for k in label2index}\n",
    "label2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "\n",
    "    \n",
    "foil = ex['gold_label']\n",
    "\n",
    "out = predictor.predict_json(ex)\n",
    "encoded_orig = out['encoded_representations']\n",
    "\n",
    "fact = out['label']\n",
    "print('Predicted: ', fact)\n",
    "\n",
    "# assert fact != foil, \"Fact should be different from the foil (if not, pick a different foil)\"\n",
    "\n",
    "ex['sentence1'] = ex['sentence1'].split()\n",
    "ex['sentence2'] = ex['sentence2'].split()\n",
    "\n",
    "tok.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "masks2 = list(all_consecutive_masks2(ex['sentence2'], max_length=1))\n",
    "encoded = []\n",
    "mask_mapping = []\n",
    "preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "for m1_i, m1 in enumerate(masks1):\n",
    "    masked1 = list(ex['sentence1'])\n",
    "    for i in m1:\n",
    "        masked1[i] = '<mask>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "        \n",
    "    for m2_i, m2 in enumerate(masks2):\n",
    "        masked2 = list(ex['sentence2'])\n",
    "        for i in m2:\n",
    "            masked2[i] = '<mask>'\n",
    "        masked2 = ' '.join(masked2)\n",
    "            \n",
    "        masked_ex = {\n",
    "            \"sentence1\": masked1,\n",
    "            \"sentence2\": masked2\n",
    "        }\n",
    "        \n",
    "        masked_out = predictor.predict_json(masked_ex)\n",
    "#         if masked_out['label'] != foil:\n",
    "#             continue\n",
    "        \n",
    "        print(m1_i, m2_i)\n",
    "        print(f\"{masked1}\\n{masked2}\")\n",
    "        print(masked_out['label'])\n",
    "        encoded.append(masked_out['encoded_representations'])\n",
    "        mask_mapping.append((m1_i, m2_i))\n",
    "        \n",
    "        print(\"====\")\n",
    "        \n",
    "encoded = np.array(encoded)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foil = 'neutral'\n",
    "\n",
    "fact_idx = label2index[fact]\n",
    "foil_idx = label2index[foil]\n",
    "print('fact:', index2label[fact_idx])\n",
    "print('foil:', index2label[foil_idx])\n",
    "num_classifiers = 100\n",
    "\n",
    "classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "print(contrastive_projection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.stats import entropy\n",
    "from scipy.special import softmax\n",
    "\n",
    "z_all = encoded_orig \n",
    "z_h = encoded \n",
    "z_all_row = encoded_orig @ contrastive_projection\n",
    "z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "print(' '.join(ex['sentence1']))\n",
    "print(' '.join(ex['sentence2']))\n",
    "\n",
    "print(\"=========\\n=======Farthest masks:=======\")    \n",
    "    \n",
    "highlight_rankings = np.argsort(-distances)\n",
    "\n",
    "for i in range(4):\n",
    "    rank = highlight_rankings[i]\n",
    "    m1_i, m2_i = mask_mapping[rank]\n",
    "    \n",
    "    masked1 = list(ex['sentence1'])\n",
    "    for k in masks1[m1_i]:\n",
    "        masked1[k] = '<m>'\n",
    "    masked1 = ' '.join(masked1)\n",
    "    \n",
    "    masked2 = list(ex['sentence2'])\n",
    "    for k in masks2[m2_i]:\n",
    "        masked2[k] = '<m>'\n",
    "    masked2 = ' '.join(masked2)\n",
    "    \n",
    "    print(masked1)\n",
    "    print(masked2)\n",
    "    print(np.round(distances[rank], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
