{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/irs38/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file ../experiments/models/ecthr/allenai/longformer-base-4096/model.tar.gz not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m MODEL_NAME\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/longformer-base-4096\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../experiments/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m archive \u001b[38;5;241m=\u001b[39m \u001b[43mload_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/model.tar.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(archive\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     27\u001b[0m archive\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_reader\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecthr\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/models/archival.py:154\u001b[0m, in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mInstantiates an Archive from an archived `tar.gz` file.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    The weights file to use.  If unspecified, weights.th in the archive_file will be used.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# redirect to the cache, if necessary\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;241m==\u001b[39m archive_file:\n\u001b[1;32m    157\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading archive file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/common/file_utils.py:202\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, extract_archive, force_extract)\u001b[0m\n\u001b[1;32m    198\u001b[0m         extraction_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_dir, extraction_name)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# File, but it doesn't exist.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_or_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Something unknown\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munable to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_or_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as a URL or as a local path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: file ../experiments/models/ecthr/allenai/longformer-base-4096/model.tar.gz not found"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel, AutoModel, LongformerModel\n",
    "import numpy as np\n",
    "from allennlp.common.util import import_module_and_submodules as import_submodules\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from scipy.spatial import distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from scipy.special import softmax\n",
    "\n",
    "import_submodules(\"allennlp_lib\")\n",
    "\n",
    "DATASET=\"ecthr\"\n",
    "MODEL_NAME=\"nlpaueb/legal-bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "model_path = \"/home/irs38/Negative-Precedent-in-Legal-Outcome-Prediction/results/Outcome/joint_model/legal_bert/facts/7f8014c5df0f432eb3f6c551ecee9ed1/model.pt\"\n",
    "model = torch.load(model_path)\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "#archive = load_archive(model_path + '/model.tar.gz')\n",
    "#print(archive.config)\n",
    "#archive.config['dataset_reader']['type'] = 'ecthr'\n",
    "#archive.config['model']['output_hidden_states'] = True\n",
    "#model = archive.model\n",
    "#model._output_hidden_states = True\n",
    "#predictor = Predictor.from_archive(archive, 'ecthr')\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data, tokenizer, max=512):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in tqdm(data):\n",
    "        sent = \" \".join(sent)\n",
    "        sent = sent[:500000] # Speeds the process up for documents with a lot of precedent we would truncate anyway.\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            # return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append([encoded_sent.get('input_ids')])\n",
    "        attention_masks.append([encoded_sent.get('attention_mask')])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "    label2index = json.load(f)\n",
    "    index2label = {label2index[k]: k for k in label2index}\n",
    "label2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def all_masks(tokenized_text):\n",
    "    # https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    #     for i in range(1 << x):  # empty and full sets included here\n",
    "    for i in range(1, 1 << x - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def all_consecutive_masks(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x):\n",
    "        for j in range(i+1, x):\n",
    "            mask = s[:i] + s[j:]\n",
    "            if max_length > 0:\n",
    "                if j - i >= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "                \n",
    "def all_consecutive_masks2(tokenized_text, max_length = -1):\n",
    "    # WITHOUT empty and full sets!\n",
    "    s = list(range(len(tokenized_text)))\n",
    "    x = len(s)\n",
    "    for i in range(x+1):\n",
    "        for j in range(i+1, x+1):\n",
    "            mask = s[i:j]\n",
    "            if max_length > 0:\n",
    "                if j - i <= max_length:\n",
    "                    yield mask\n",
    "            else:\n",
    "                yield mask\n",
    "\n",
    "def precisionAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recallAtK(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def meanPrecisionAtK(actual, predicted, k):\n",
    "    return np.mean([precisionAtK(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def meanRecallAtK(actual, predicted, k):\n",
    "    return np.mean([recallAtK(a, p, k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m non_zero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m val_data: \n\u001b[0;32m---> 22\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     claims \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m     outcomes \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_predictor.py:41\u001b[0m, in \u001b[0;36mECtHRPredictor.predict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@overrides\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: JsonDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JsonDict:\n\u001b[1;32m     40\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_json_to_instance(inputs)\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_predictor.py:45\u001b[0m, in \u001b[0;36mECtHRPredictor.predict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@overrides\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: Instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JsonDict:\n\u001b[0;32m---> 45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sanitize(outputs)\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_classifier.py:185\u001b[0m, in \u001b[0;36mECtHRClassifier.forward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;129m@overrides\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_on_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: Instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    Takes an [`Instance`](../data/instance.md), which typically has raw text in it, converts\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    that text into arrays using this model's [`Vocabulary`](../data/vocabulary.md), passes those\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    `torch.Tensors` into numpy arrays and remove the batch dimension.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_classifier.py:212\u001b[0m, in \u001b[0;36mECtHRClassifier.forward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    210\u001b[0m dataset\u001b[38;5;241m.\u001b[39mindex_instances(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m    211\u001b[0m model_input \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mmove_to_device(dataset\u001b[38;5;241m.\u001b[39mas_tensor_dict(), cuda_device)\n\u001b[0;32m--> 212\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_output_human_readable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    214\u001b[0m instance_separated_output: List[Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    215\u001b[0m     {} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39minstances\n\u001b[1;32m    216\u001b[0m ]\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/contrastive-explanations/allennlp_lib/ecthr_classifier.py:142\u001b[0m, in \u001b[0;36mECtHRClassifier.forward\u001b[0;34m(self, facts, labels)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     facts: TextFieldTensors,\n\u001b[1;32m    118\u001b[0m     labels: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    # Parameters\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m            A scalar loss to be optimised.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     embedded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_field_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfacts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     mask \u001b[38;5;241m=\u001b[39m get_text_field_mask(facts)\n\u001b[1;32m    145\u001b[0m     global_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    146\u001b[0m         facts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py:88\u001b[0m, in \u001b[0;36mBasicTextFieldEmbedder.forward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     token_vectors \u001b[38;5;241m=\u001b[39m embedder(\u001b[38;5;28mlist\u001b[39m(tensors\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params_values)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# If there are multiple tensor arguments, we have to require matching names from the\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# TokenIndexer.  I don't think there's an easy way around that.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     token_vectors \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_vectors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# To handle some very rare use cases, we allow the return value of the embedder to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# be None; we just skip it in that case.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     embedded_representations\u001b[38;5;241m.\u001b[39mappend(token_vectors)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:184\u001b[0m, in \u001b[0;36mPretrainedTransformerEmbedder.forward\u001b[0;34m(self, token_ids, mask, type_ids, segment_concat_mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m type_ids\n\u001b[0;32m--> 184\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scalar_mix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# As far as I can tell, the hidden states will always be the last element\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# in the output tuple as long as the model is not also configured to return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# These hidden states will also include the embedding layer, which we don't\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# include in the scalar mix. Hence the `[1:]` slicing.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1594\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1586\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape, device)[\n\u001b[1;32m   1587\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[1;32m   1588\u001b[0m ]\n\u001b[1;32m   1590\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1591\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m   1592\u001b[0m )\n\u001b[0;32m-> 1594\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1602\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1223\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1216\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1217\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         is_index_global_attn,\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1223\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1158\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_masked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m ):\n\u001b[0;32m-> 1158\u001b[0m     self_attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1166\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1103\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_masked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_index_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_global_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m ):\n\u001b[0;32m-> 1103\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m   1111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/torch/nn/modules/module.py:722\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    724\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    726\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:644\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m local_attn_probs_fp32\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# softmax sometimes inserts NaN if all positions are masked, replace them with 0\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m local_attn_probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_attn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# apply dropout\u001b[39;00m\n\u001b[1;32m    647\u001b[0m local_attn_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(local_attn_probs, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#read in the validation data, which is a json dict in each new line\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/simple_val.jsonl\", \"r\") as f:\n",
    "    val_data = [json.loads(line) for line in f.readlines()]\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/dev.jsonl\", \"r\") as f:\n",
    "    val_meta_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for item in val_data: \n",
    "    item[\"facts_sentences\"] = [i for i in val_meta_data if i[\"case_no\"] == item[\"case_no\"]][0][\"facts\"]\n",
    "\n",
    "articles = ['10', '11', '13', '14', '18', '2', '3', '4', '5', '6', '7', '8', '9', 'P1-1', 'P4-2', 'P7-1', 'P7-4']\n",
    "\n",
    "#ex = {\"facts\": \"5.  The applicant was born in 1983 and is detained in Sztum. 6.  At the time of the events in question, the applicant was serving a prison sentence in the Barczewo prison. 7.  On 8 January 2011 the applicant\\u2019s grandmother died. On 10 January 2011 the applicant lodged a request with the Director of Prison and the Penitentiary judge for leave to attend her funeral which was to take place on 12 January 2011. Together with his application he submitted a statement from his sister E.K. who confirmed that she would personally collect the applicant from prison and bring him back after the funeral. 8.  On 11 January 2011 the Penitentiary judge of the Olsztyn Regional Court (S\\u0119dzia Penitencjarny S\\u0105du Okr\\u0119gowego w Olsztynie) allowed the applicant to attend the funeral under prison officers\\u2019 escort. The reasoning of the decision read as follows:\\n\\u201cIn view of [the applicant\\u2019s] multiple convictions and his long term of imprisonment there is no guarantee that he will return to prison\\u201d 9.  The applicant refused to attend the funeral, since he believed his appearance under escort of uniformed officers would create a disturbance during the ceremony. 10.  On the same day the applicant lodged an appeal with the Olsztyn Regional Court (S\\u0105d Okr\\u0119gowy) complaining that the compassionate leave was granted under escort and also that he was only allowed to participate in the funeral (not the preceding church service). 11.  On 3 February 2011 the Olsztyn Regional Court upheld the Penitentiary judge\\u2019s decision and dismissed the appeal. The court stressed that the applicant had been allowed to participate in the funeral under prison officers\\u2019 escort. It further noted that the applicant was a habitual offender sentenced to a long term of imprisonment therefore there was no positive criminological prognosis and no guarantee that he would have returned to prison after the ceremony.\", \"claims\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"20488/11\"}\n",
    "#ex = {\"facts\": \"4.  The applicant was born in 1960 and lives in Oleksandrivka, Kirovograd Region. 5.  On 3 February 2007 the applicant was assaulted. According to the subsequent findings of medical experts, she sustained haematomas on her jaw, shoulder and hip, a bruise under her right eye, concussion, and a displaced rib fracture. The applicant alleges that her assailants were Mr and Mrs K., her daughter\\u2019s former parents-in-law, whereas the domestic authorities found that it was only Mrs K. who had assaulted the applicant. The incident occurred in front of the applicant\\u2019s two-year-old granddaughter. 6.  On 4 February 2007 the applicant lodged a complaint with the police. 7.  On 5 February 2007 a forensic medical expert examined the applicant. He found that she had haematomas which he classified as \\u201cminor bodily injuries\\u201d. 8.  On 14 February 2007 the Oleksandrivka District Police Department (\\u201cthe Oleksandrivka police\\u201d) refused to institute criminal proceedings in connection with the incident. 9.  On 22 February 2007 a forensic medical examination of the applicant was carried out. The expert found that in addition to the previously noted haematomas, the applicant had also suffered concussion and a displaced rib fracture. The expert classified the injuries as \\u201cbodily harm of medium severity\\u201d. 10.  On 20 March 2007 the Oleksandrivka prosecutor overruled the decision of 14 February 2007 as premature and on 21 March 2007 instituted criminal proceedings in connection with the infliction of bodily harm of medium severity on the applicant. 11.  On 20 May 2007 the investigator suspended the investigation for failure to identify the perpetrator. 12.  On 29 August and 3 October 2007 the Oleksandrivka prosecutor\\u2019s office issued two decisions in which it overruled the investigator\\u2019s decision of 20 May 2007 as premature. 13.  On 6 October 2007 the investigator questioned Mr and Mrs K. 14.  On 1 December 2007 the investigator again suspended the investigation for failure to identify the perpetrator. 15.  On 10 December 2007 the Oleksandrivka prosecutor\\u2019s office, in response to the applicant\\u2019s complaint about the progress of the investigation, asked the Kirovograd Regional Police Department to have the police officers in charge of the investigation disciplined. 16.  On 21 January 2008 the Kirovograd Regional Police Department instructed the Oleksandrivka police to immediately resume the investigation. 17.  On 7 April 2008 the investigator decided to ask a forensic medical expert to determine the degree of gravity of the applicant\\u2019s injuries. On 22 September 2008 the expert drew up a report generally confirming the findings of 22 February 2007. 18.  On 15 May 2008 the Kirovograd Regional Police Department informed the applicant that the police officers in charge of the case had been disciplined for omissions in the investigation. 19.  On 23 October 2008 the Oleksandrivka Court absolved Mrs K. from criminal liability under an amnesty law, on the grounds that she had an elderly mother who was dependent on her. On 24 February 2009 the Kirovograd Regional Court of Appeal (\\u201cthe Court of Appeal\\u201d) quashed that judgment, finding no evidence that Mrs K.\\u2019s mother was dependent on her. 20.  On 1 July 2009 the investigator refused to institute criminal proceedings against Mr K. 21.  On 7 July 2009 the Novomyrgorod prosecutor issued a bill of indictment against Mrs K. 22.  On 24 July 2009 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding that the applicant had not been informed about the completion of the investigation until 3 July 2009 and had therefore not been given enough time to study the case file. It also held that the refusal to institute criminal proceedings against Mr K. had contravened the law. 23.  On 13 November 2009 the Novomyrgorod prosecutor quashed the decision of 1 July 2009 not to institute criminal proceedings against Mr K. Subsequently the investigator again refused to institute criminal proceedings against Mr K. 24.  On 21 December 2009 the new round of pre-trial investigation in the case against Mrs K. was completed and another bill of indictment was issued by the Novomyrgorod prosecutor. 25.  On 29 March 2010 the Oleksandrivka Court remitted the case against Mrs K. for further investigation, holding in particular that the decision not to institute criminal proceedings against Mr K. had been premature, since his role in the incident had not been sufficiently clarified. 26.  On 13 July 2010 the Novomyrgorod prosecutor quashed the decision not to institute criminal proceedings against Mr K. On 26 May 2011 the investigator again refused to institute criminal proceedings against Mr K. 27.  On 20 December 2011 the Znamyanka Court convicted Mrs K. of inflicting bodily harm of medium severity on the applicant, sentencing her to restriction of liberty for two years, suspended for a one-year probationary period. The court found that the decision not to institute criminal proceedings against Mr K. in connection with the same incident had been correct. Mrs K., the prosecutor and the applicant appealed. 28.  On 6 March 2012 the Court of Appeal quashed the judgment and discontinued the criminal proceedings against Mrs K. as time-barred.\", \"claims\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"outcomes\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"case_no\": \"27454/11\"}\n",
    "\n",
    "#shuffle val_data\n",
    "random.shuffle(val_data)\n",
    "\n",
    "interesting_items = []\n",
    "\n",
    "non_zero = 0\n",
    "for e in val_data: \n",
    "    out = predictor.predict_json(e)\n",
    "    claims = e[\"claims\"]\n",
    "    outcomes = e[\"outcomes\"]\n",
    "    gold = [\"not_claimed\" if c == 0 else \"claimed_not_violated\" if c == 1 and o == 0 else \"claimed_and_violated\" for c, o in zip(claims, outcomes)]\n",
    "    gold_id = e[\"case_no\"]\n",
    "    silver_rationales = [i for i in val_meta_data if i[\"case_no\"] == gold_id][0][\"silver_rationales\"]\n",
    "    if len(set(out[\"labels\"])) != 1 and out[\"labels\"] == gold and silver_rationales:\n",
    "        #print(out[\"labels\"])\n",
    "        non_zero += 1\n",
    "        ex = e\n",
    "        interesting_items.append({\"out\":out, \"claims\":claims, \"outcomes\":outcomes, \"ex\":ex, \"gold\":gold, \"silver_rationales\":silver_rationales})\n",
    "        #break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interesting_items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m all_interesting_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interesting_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43minteresting_items\u001b[49m: \n\u001b[1;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m interesting_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     claims \u001b[38;5;241m=\u001b[39m interesting_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interesting_items' is not defined"
     ]
    }
   ],
   "source": [
    "all_interesting_results = []\n",
    "\n",
    "for interesting_item in interesting_items: \n",
    "    out = interesting_item[\"out\"]\n",
    "    claims = interesting_item[\"claims\"]\n",
    "    outcomes = interesting_item[\"outcomes\"]\n",
    "    ex = interesting_item[\"ex\"]\n",
    "    gold = interesting_item[\"gold\"]\n",
    "    silver_rationales = interesting_item[\"silver_rationales\"]\n",
    "    \n",
    "    encoded_orig = out['encoded_representations']\n",
    "\n",
    "    facts = out['labels']\n",
    "    #print('Predicted: ', facts)\n",
    "\n",
    "    tokenizer.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "\n",
    "    facts_sentences = ex[\"facts_sentences\"]\n",
    "\n",
    "    masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "    masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "    encoded = []\n",
    "    mask_mapping = []\n",
    "    preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "    for m1_i, m1 in enumerate(masks1):\n",
    "        masked1 = []\n",
    "        for i in m1:\n",
    "            masked1[i] = '<mask>'\n",
    "        masked1 = ' '.join(masked1)\n",
    "        masked_sentence = []\n",
    "        for m2_i, m2 in enumerate(masks2):\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for i in m2:\n",
    "                masked_sentence.append(masked2[i])\n",
    "                sentence_length = len(tok.tokenize(masked2[i]))\n",
    "                masked2[i] = '<mask> '*sentence_length\n",
    "            masked2 = tok.tokenize(' '.join(masked2))\n",
    "                \n",
    "            masked_ex = {\n",
    "                \"facts\": masked2,\n",
    "                \"claims\": claims,\n",
    "                \"outcomes\": outcomes,\n",
    "                \"case_no\": ex['case_no']\n",
    "            }\n",
    "            \n",
    "            masked_out = predictor.predict_json(masked_ex)\n",
    "\n",
    "            #print(\"indices\", m1_i, m2_i)\n",
    "            #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "            #print(\"gold labels\", masked_out['labels'])\n",
    "            #print(\"masked out sentence\", masked_sentence)\n",
    "            encoded.append(masked_out['encoded_representations'])\n",
    "            mask_mapping.append((m1_i, m2_i))\n",
    "            \n",
    "            #print(\"====\")\n",
    "            \n",
    "    encoded = np.array(encoded)\n",
    "\n",
    "    # replace some random f in the following list with another option from\n",
    "    # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "    label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    interesting_label_options = [\"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "    article_id = random.choice([i for i in range(len(facts)) if facts[i] in interesting_label_options or gold[i] in interesting_label_options])\n",
    "    foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(facts)]\n",
    "\n",
    "    fact_idx = label2index[facts[article_id]]\n",
    "    foil_idx = label2index[foils[article_id]]\n",
    "    #print(\"article number\", articles[article_id])\n",
    "    #print('fact:', index2label[fact_idx])\n",
    "    #print('foil:', index2label[foil_idx])\n",
    "\n",
    "    fact_idx = article_id * len(label_options) + fact_idx\n",
    "    foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "    classifier_w = model_state_dict[\"classifier.3.weight\"].numpy()\n",
    "    classifier_b = model_state_dict[\"classifier.3.bias\"].numpy()\n",
    "    #classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "    #classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "    u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "    contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "    #print(contrastive_projection.shape)\n",
    "\n",
    "    z_all = encoded_orig \n",
    "    z_h = encoded \n",
    "    z_all_row = encoded_orig @ contrastive_projection\n",
    "    z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "    prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "    prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "    prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "    p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "    q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "    p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "    q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "    distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "    #print(\"the case\", ex['facts'])\n",
    "    #print(\"silver rationales\", silver_rationales)\n",
    "    #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "        \n",
    "    highlight_rankings = np.argsort(-distances)\n",
    "    explained_indices = []\n",
    "\n",
    "    for i in range(len(facts_sentences)):\n",
    "        rank = highlight_rankings[i]\n",
    "        m1_i, m2_i = mask_mapping[rank]\n",
    "        \n",
    "        masked_sentence = []\n",
    "        masked2 = facts_sentences.copy()\n",
    "        for k in masks2[m2_i]:\n",
    "            masked_sentence.append(masked2[k])\n",
    "            masked2[k] = '<mask>'\n",
    "        explained_indices.append(k)\n",
    "        masked2 = ' '.join(masked2)\n",
    "        #print(\"input with sentence masked out \\n\",masked2)\n",
    "        #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "        #print(\"omitted index\\n\", i)\n",
    "        #print(np.round(distances[rank], 4))\n",
    "        \n",
    "    #print(explained_indices)\n",
    "    all_interesting_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "    print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "985\n",
      "meanPrecision@ 2   0.9022988505747126\n",
      "meanRecall@ 2   0.14178598326617356\n",
      "meanPrecision@ 3   0.7892720306513408\n",
      "meanRecall@ 3   0.1763480453128247\n",
      "meanPrecision@ 4   0.7155172413793104\n",
      "meanRecall@ 4   0.2056083138118052\n",
      "meanPrecision@ 5   0.6505747126436783\n",
      "meanRecall@ 5   0.22842611003506907\n",
      "meanPrecision@ 6   0.6034482758620688\n",
      "meanRecall@ 6   0.24904954889156178\n",
      "meanPrecision@ 7   0.5533661740558292\n",
      "meanRecall@ 7   0.2615054696612314\n",
      "meanPrecision@ 8   0.5100574712643678\n",
      "meanRecall@ 8   0.271469390094073\n",
      "meanPrecision@ 9   0.47126436781609204\n",
      "meanRecall@ 9   0.2781305787759071\n"
     ]
    }
   ],
   "source": [
    "print(len(all_interesting_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_interesting_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_interesting_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "985\n",
      "meanPrecision@ 2   nan\n",
      "meanRecall@ 2   nan\n",
      "meanPrecision@ 3   nan\n",
      "meanRecall@ 3   nan\n",
      "meanPrecision@ 4   nan\n",
      "meanRecall@ 4   nan\n",
      "meanPrecision@ 5   nan\n",
      "meanRecall@ 5   nan\n",
      "meanPrecision@ 6   nan\n",
      "meanRecall@ 6   nan\n",
      "meanPrecision@ 7   nan\n",
      "meanRecall@ 7   nan\n",
      "meanPrecision@ 8   nan\n",
      "meanRecall@ 8   nan\n",
      "meanPrecision@ 9   nan\n",
      "meanRecall@ 9   nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/irs38/.conda/envs/contrastive/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         incorrect_items\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m:out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m:claims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcomes\u001b[39m\u001b[38;5;124m\"\u001b[39m:outcomes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\"\u001b[39m:ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold\u001b[39m\u001b[38;5;124m\"\u001b[39m:gold, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_rationales\u001b[39m\u001b[38;5;124m\"\u001b[39m:silver_rationales})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# save incorrect items\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../experiments/ecthr/incorrect_items_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m incorrect_items:\n\u001b[1;32m     18\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(item) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/contrastive/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/ecthr/incorrect_items_allenai/longformer-base-4096.jsonl'"
     ]
    }
   ],
   "source": [
    "incorrect_items = []\n",
    "non_zero = 0\n",
    "for e in val_data: \n",
    "    out = predictor.predict_json(e)\n",
    "    claims = e[\"claims\"]\n",
    "    outcomes = e[\"outcomes\"]\n",
    "    gold = [\"not_claimed\" if c == 0 else \"claimed_not_violated\" if c == 1 and o == 0 else \"claimed_and_violated\" for c, o in zip(claims, outcomes)]\n",
    "    gold_id = e[\"case_no\"]\n",
    "    silver_rationales = [i for i in val_meta_data if i[\"case_no\"] == gold_id][0][\"silver_rationales\"]\n",
    "    if out[\"labels\"] != gold and silver_rationales:\n",
    "        non_zero += 1\n",
    "        ex = e\n",
    "        incorrect_items.append({\"out\":out, \"claims\":claims, \"outcomes\":outcomes, \"ex\":ex, \"gold\":gold, \"silver_rationales\":silver_rationales})\n",
    "\n",
    "# save items from the incorrect_items list to a file\n",
    "with open(f\"./incorrect_items.txt\", \"w\") as f:\n",
    "    for item in incorrect_items:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# if the file f\"./incorrect_results.txt\" exists then remove it\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    os.remove(f\"./incorrect_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "985\n",
      "meanPrecision@ 2   0.9477739726027398\n",
      "meanRecall@ 2   0.09767005583962775\n",
      "meanPrecision@ 3   0.8978310502283103\n",
      "meanRecall@ 3   0.135352180169421\n",
      "meanPrecision@ 4   0.8441780821917808\n",
      "meanRecall@ 4   0.16501159742012056\n",
      "meanPrecision@ 5   0.7948630136986301\n",
      "meanRecall@ 5   0.1890894937220949\n",
      "meanPrecision@ 6   0.7471461187214612\n",
      "meanRecall@ 6   0.2077014659264444\n",
      "meanPrecision@ 7   0.7045009784735813\n",
      "meanRecall@ 7   0.2227548321363115\n",
      "meanPrecision@ 8   0.6673801369863014\n",
      "meanRecall@ 8   0.23588981179558793\n",
      "meanPrecision@ 9   0.6322298325722984\n",
      "meanRecall@ 9   0.2463872776379505\n"
     ]
    }
   ],
   "source": [
    "# read in incorrect items\n",
    "with open(f\"./incorrect_items.txt\", \"r\") as f:\n",
    "    incorrect_items = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "#check if a file in the path ./incorrect_results.txt exists\n",
    "if os.path.exists(f\"./incorrect_results.txt\"):\n",
    "    # load incorrect results from the incorrect_results file\n",
    "    with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "        incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "else: \n",
    "    incorrect_results = []\n",
    "    \n",
    "saved_exes = [i[\"ex\"] for i in incorrect_results]\n",
    "\n",
    "all_incorrect_results = []\n",
    "\n",
    "for incorrect_item in incorrect_items: \n",
    "    out = incorrect_item[\"out\"]\n",
    "    claims = incorrect_item[\"claims\"]\n",
    "    outcomes = incorrect_item[\"outcomes\"]\n",
    "    ex = incorrect_item[\"ex\"]\n",
    "    gold = incorrect_item[\"gold\"]\n",
    "    silver_rationales = incorrect_item[\"silver_rationales\"]\n",
    "    \n",
    "    if ex not in saved_exes:\n",
    "\n",
    "        encoded_orig = out['encoded_representations']\n",
    "\n",
    "        facts = out['labels']\n",
    "        #print('Predicted: ', facts)\n",
    "\n",
    "        tok.convert_tokens_to_string(out['tokens'])\n",
    "\n",
    "\n",
    "        facts_sentences = ex[\"facts_sentences\"]\n",
    "\n",
    "        masks1 = [[]]  # change this if you also want to mask out parts of the premise.\n",
    "        masks2 = list(all_consecutive_masks2(facts_sentences, max_length=1))\n",
    "        encoded = []\n",
    "        mask_mapping = []\n",
    "        preds = np.zeros(shape=(len(masks1), len(masks2)))\n",
    "\n",
    "        for m1_i, m1 in enumerate(masks1):\n",
    "            masked1 = []\n",
    "            for i in m1:\n",
    "                masked1[i] = '<mask>'\n",
    "            masked1 = ' '.join(masked1)\n",
    "            masked_sentence = []\n",
    "            for m2_i, m2 in enumerate(masks2):\n",
    "                masked2 = facts_sentences.copy()\n",
    "                for i in m2:\n",
    "                    masked_sentence.append(masked2[i])\n",
    "                    sentence_length = len(tok.tokenize(masked2[i]))\n",
    "                    masked2[i] = '<mask> '*sentence_length\n",
    "                masked2 = tok.tokenize(' '.join(masked2))\n",
    "                    \n",
    "                masked_ex = {\n",
    "                    \"facts\": masked2,\n",
    "                    \"claims\": claims,\n",
    "                    \"outcomes\": outcomes,\n",
    "                    \"case_no\": ex['case_no']\n",
    "                }\n",
    "                \n",
    "                masked_out = predictor.predict_json(masked_ex)\n",
    "\n",
    "                #print(\"indices\", m1_i, m2_i)\n",
    "                #print(\"case facts with masks in them\", f\"{masked1}\\n{masked2}\")\n",
    "                #print(\"gold labels\", masked_out['labels'])\n",
    "                #print(\"masked out sentence\", masked_sentence)\n",
    "                encoded.append(masked_out['encoded_representations'])\n",
    "                mask_mapping.append((m1_i, m2_i))\n",
    "                \n",
    "                #print(\"====\")\n",
    "            \n",
    "        encoded = np.array(encoded)\n",
    "\n",
    "        # replace some random f in the following list with another option from\n",
    "        # [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"] at random\n",
    "        label_options = [\"not_claimed\", \"claimed_and_violated\", \"claimed_not_violated\"]\n",
    "        article_id = random.choice([i for i in range(len(facts))])\n",
    "        foils = [f if i != article_id else random.choice([o for o in label_options if o != f]) for i,f in enumerate(facts)]\n",
    "\n",
    "        fact_idx = label2index[facts[article_id]]\n",
    "        foil_idx = label2index[foils[article_id]]\n",
    "        #print(\"article number\", articles[article_id])\n",
    "        #print('fact:', index2label[fact_idx])\n",
    "        #print('foil:', index2label[foil_idx])\n",
    "\n",
    "        fact_idx = article_id * len(label_options) + fact_idx\n",
    "        foil_idx = article_id * len(label_options) + foil_idx\n",
    "\n",
    "        classifier_w = np.load(f\"{model_path}/w.npy\")\n",
    "        classifier_b = np.load(f\"{model_path}/b.npy\")\n",
    "\n",
    "        u = classifier_w[fact_idx] - classifier_w[foil_idx]\n",
    "        contrastive_projection = np.outer(u, u) / np.dot(u, u)\n",
    "\n",
    "        #print(contrastive_projection.shape)\n",
    "\n",
    "        z_all = encoded_orig \n",
    "        z_h = encoded \n",
    "        z_all_row = encoded_orig @ contrastive_projection\n",
    "        z_h_row = encoded @ contrastive_projection\n",
    "\n",
    "        prediction_probabilities = softmax(z_all_row @ classifier_w.T + classifier_b)\n",
    "        prediction_probabilities = np.tile(prediction_probabilities, (z_h_row.shape[0], 1))\n",
    "\n",
    "        prediction_probabilities_del = softmax(z_h_row @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "        p = prediction_probabilities[:, [fact_idx, foil_idx]]\n",
    "        q = prediction_probabilities_del[:, [fact_idx, foil_idx]]\n",
    "\n",
    "        p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "        q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "        distances = (p[:, 0] - q[:, 0])\n",
    "\n",
    "        #print(\"the case\", ex['facts'])\n",
    "        #print(\"silver rationales\", silver_rationales)\n",
    "        #print(\"=========\\n=======Farthest masks:=======\")    \n",
    "            \n",
    "        highlight_rankings = np.argsort(-distances)\n",
    "        explained_indices = []\n",
    "\n",
    "        for i in range(len(facts_sentences)):\n",
    "            rank = highlight_rankings[i]\n",
    "            m1_i, m2_i = mask_mapping[rank]\n",
    "            \n",
    "            masked_sentence = []\n",
    "            masked2 = facts_sentences.copy()\n",
    "            for k in masks2[m2_i]:\n",
    "                masked_sentence.append(masked2[k])\n",
    "                masked2[k] = '<mask>'\n",
    "            explained_indices.append(k)\n",
    "            masked2 = ' '.join(masked2)\n",
    "            #print(\"input with sentence masked out \\n\",masked2)\n",
    "            #print(\"the sentence that has been omitted\\n\", masked_sentence)\n",
    "            #print(\"omitted index\\n\", i)\n",
    "            #print(np.round(distances[rank], 4))\n",
    "            \n",
    "        #print(explained_indices)\n",
    "        all_incorrect_results.append({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        print({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices})\n",
    "        # append incorrect result to an external incorrect_results file\n",
    "        with open(f\"./incorrect_results.txt\", \"a\") as f:\n",
    "            f.write(json.dumps({\"ex\":ex, \"silver_rationales\":silver_rationales, \"explained_indices\":explained_indices}) + \"\\n\")\n",
    "\n",
    "# read in all_incorrect_results from the incorrect_results file\n",
    "with open(f\"./incorrect_results.txt\", \"r\") as f:\n",
    "    all_incorrect_results = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"/home/irs38/contrastive-explanations/data/ecthr/Chalkidis/simple_val.jsonl\", \"r\") as f:\n",
    "    val_data = [json.loads(line) for line in f.readlines()]       \n",
    "\n",
    "print(len(all_incorrect_results))\n",
    "print(len(val_data))\n",
    "actual = [a[\"explained_indices\"] for a in all_incorrect_results]\n",
    "predicted = [p[\"silver_rationales\"] for p in all_incorrect_results]\n",
    "for i in range(2, 10):\n",
    "    print(\"meanPrecision@\", i, \" \", meanPrecisionAtK(actual, predicted, i))\n",
    "    print(\"meanRecall@\", i, \" \", meanRecallAtK(actual, predicted, i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
